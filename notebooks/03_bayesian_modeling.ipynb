{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1c02c6a",
   "metadata": {},
   "source": [
    "# Bayesian RCT Lean Modeling\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This section implements the **Bayesian RCT Lean modeling approach** for predicting control arm outcomes with uncertainty quantification. Unlike the point estimate approach in the main notebook, this provides full posterior distributions for:\n",
    "\n",
    "- **Control arm predictions**: Complete uncertainty around predicted outcomes\n",
    "- **ATE estimates**: Probabilistic treatment effect predictions\n",
    "- **Clinical decision making**: Risk-based trial go/no-go decisions\n",
    "\n",
    "### Key Advantages of Bayesian Approach:\n",
    "- **Uncertainty quantification**: Know confidence in predictions\n",
    "- **Risk assessment**: Calculate probability of positive/negative effects\n",
    "- **Sequential learning**: Update beliefs as new trial data arrives\n",
    "- **Clinical interpretation**: Natural probabilistic language for clinicians"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbdd44d",
   "metadata": {},
   "source": [
    "## Load RCT Data and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdbb0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pymc as pm\n",
    "import arviz as az\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from scipy import stats\n",
    "import json\n",
    "import os\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e8e9e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import utility functions\n",
    "from utilities.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f16dabe7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NCT_ID</th>\n",
       "      <th>Arm</th>\n",
       "      <th>Population</th>\n",
       "      <th>intervention</th>\n",
       "      <th>RCT_with_control_inter</th>\n",
       "      <th>gender_male_percent</th>\n",
       "      <th>age_median</th>\n",
       "      <th>no_smoker_percent</th>\n",
       "      <th>ecog_1</th>\n",
       "      <th>brain_metastase_yes</th>\n",
       "      <th>...</th>\n",
       "      <th>is_arm_control</th>\n",
       "      <th>combo_therapy</th>\n",
       "      <th>egfr_targeted</th>\n",
       "      <th>egfr_tki_use</th>\n",
       "      <th>high_risk_profile</th>\n",
       "      <th>novelty_score</th>\n",
       "      <th>elderly_male</th>\n",
       "      <th>large_trial</th>\n",
       "      <th>treatment_complexity</th>\n",
       "      <th>smoker_percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NCT01364012</td>\n",
       "      <td>Intervention</td>\n",
       "      <td>138.0</td>\n",
       "      <td>bevacizumab + platinum doublet chemo (carbopla...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NCT01364012</td>\n",
       "      <td>Control</td>\n",
       "      <td>138.0</td>\n",
       "      <td>placebo + platinum doublet chemo (carboplatin ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NCT01469000</td>\n",
       "      <td>Intervention</td>\n",
       "      <td>126.0</td>\n",
       "      <td>pemetrexed + gefitinib</td>\n",
       "      <td>1.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>36.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NCT01469000</td>\n",
       "      <td>Control</td>\n",
       "      <td>65.0</td>\n",
       "      <td>gefitinib</td>\n",
       "      <td>1.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>28.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NCT02099058</td>\n",
       "      <td>Intervention</td>\n",
       "      <td>28.0</td>\n",
       "      <td>telisotuzumab vedotin + erlotinib</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        NCT_ID           Arm  Population  \\\n",
       "0  NCT01364012  Intervention       138.0   \n",
       "1  NCT01364012       Control       138.0   \n",
       "2  NCT01469000  Intervention       126.0   \n",
       "3  NCT01469000       Control        65.0   \n",
       "4  NCT02099058  Intervention        28.0   \n",
       "\n",
       "                                        intervention  RCT_with_control_inter  \\\n",
       "0  bevacizumab + platinum doublet chemo (carbopla...                     1.0   \n",
       "1  placebo + platinum doublet chemo (carboplatin ...                     1.0   \n",
       "2                            pemetrexed + gefitinib                      1.0   \n",
       "3                                          gefitinib                     1.0   \n",
       "4                  telisotuzumab vedotin + erlotinib                     0.0   \n",
       "\n",
       "   gender_male_percent  age_median  no_smoker_percent  ecog_1  \\\n",
       "0                 54.0        57.0               50.0    75.0   \n",
       "1                 56.0        56.0               50.0    80.0   \n",
       "2                 35.0        62.0               64.0    69.0   \n",
       "3                 37.0        62.0               72.0    68.0   \n",
       "4                 32.0        60.0                0.0    71.0   \n",
       "\n",
       "   brain_metastase_yes  ...  is_arm_control  combo_therapy  egfr_targeted  \\\n",
       "0                  0.0  ...               0              1            0.0   \n",
       "1                  0.0  ...               1              0            0.0   \n",
       "2                  0.0  ...               0              1          100.0   \n",
       "3                  0.0  ...               1              0          100.0   \n",
       "4                  0.0  ...               0              0          100.0   \n",
       "\n",
       "   egfr_tki_use  high_risk_profile  novelty_score elderly_male  large_trial  \\\n",
       "0           0.0                  1            1.0            0            0   \n",
       "1           0.0                  1            0.0            0            0   \n",
       "2         100.0                  1            1.0            0            0   \n",
       "3         100.0                  1            1.0            0            0   \n",
       "4         100.0                  1            1.0            0            0   \n",
       "\n",
       "   treatment_complexity  smoker_percent  \n",
       "0                   3.0            50.0  \n",
       "1                   1.0            50.0  \n",
       "2                   2.0            36.0  \n",
       "3                   1.0            28.0  \n",
       "4                   2.0           100.0  \n",
       "\n",
       "[5 rows x 45 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load RCT training data\n",
    "training_df = pd.read_csv(\"processed_training_data.csv\")\n",
    "training_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f42246a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load feature configuration\n",
    "config_path = os.path.join('config', 'feature_config.json')\n",
    "with open(config_path, 'r') as f:\n",
    "    feature_config = json.load(f)\n",
    "\n",
    "\n",
    "# Extract configuration\n",
    "target_col = feature_config.get('target', 'PFS_median_months')\n",
    "trial_id_col = feature_config.get('trial_id_column', 'NCT_ID')\n",
    "control_arm_col = feature_config.get('control_arm_column', 'is_arm_control')\n",
    "intervention_outcome_col = feature_config.get('intervention_outcome_column', 'intervention_outcome')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4952b494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (57, 45)\n",
      "Number of trials: 27\n",
      "Bayesian features: ['age_median', 'gender_male_percent', 'ecog_1', 'EGFR_positive_mutation', 'disease_stage_IV', 'PD1_PDL1_Inhibitor', 'EGFR_TKI', 'Platinum_Chemotherapy']\n"
     ]
    }
   ],
   "source": [
    "# Use subset of most important features for Bayesian modeling\n",
    "bayesian_features = [\n",
    "    'age_median', 'gender_male_percent', 'ecog_1', \n",
    "    'EGFR_positive_mutation', 'disease_stage_IV',\n",
    "    'PD1_PDL1_Inhibitor', 'EGFR_TKI', 'Platinum_Chemotherapy'\n",
    "]\n",
    "\n",
    "print(f\"Dataset shape: {training_df.shape}\")\n",
    "print(f\"Number of trials: {training_df[trial_id_col].nunique()}\")\n",
    "print(f\"Bayesian features: {bayesian_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44492c3c",
   "metadata": {},
   "source": [
    "## Bayesian Control Arm Prediction Model\n",
    "\n",
    "### Model Specification:\n",
    "\n",
    "We build a hierarchical Bayesian model for control arm outcomes:\n",
    "\n",
    "```\n",
    "PFS_control ~ Normal(μ, σ)\n",
    "μ = α + Σ(βᵢ × featureᵢ) + β_intervention × PFS_intervention\n",
    "```\n",
    "\n",
    "**Priors:**\n",
    "- Weakly informative priors based on clinical knowledge\n",
    "- Regularization through prior specification\n",
    "- **No standardization**: Mixed feature types (continuous + one-hot encoded)\n",
    "- **Missing value handling**: Using existing prepare_data and run_fillna functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a725cde4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw data shape: (24, 46)\n",
      "Features to use: ['age_median', 'gender_male_percent', 'ecog_1', 'EGFR_positive_mutation', 'disease_stage_IV', 'PD1_PDL1_Inhibitor', 'EGFR_TKI', 'Platinum_Chemotherapy']\n",
      "Prepared data shape: (24, 11)\n",
      "Missing values after preparation: 0\n",
      "\n",
      "Final Bayesian modeling dataset:\n",
      "Shape: (24, 11)\n",
      "Columns: ['age_median', 'gender_male_percent', 'ecog_1', 'EGFR_positive_mutation', 'disease_stage_IV', 'PD1_PDL1_Inhibitor', 'EGFR_TKI', 'Platinum_Chemotherapy', 'intervention_outcome', 'PFS_median_months', 'NCT_ID']\n",
      "Target variable stats: mean=6.32, std=1.77\n"
     ]
    }
   ],
   "source": [
    "def prepare_bayesian_data(df, features, target_col, trial_id_col, control_arm_col):\n",
    "    \"\"\"Prepare data for Bayesian modeling with proper missing value handling\"\"\"\n",
    "    \n",
    "    # Start with control arms only\n",
    "    control_data = df[df[control_arm_col] == 1].copy()\n",
    "    \n",
    "    # Add intervention outcomes for each control arm\n",
    "    control_data[intervention_outcome_col] = np.nan\n",
    "    \n",
    "    for idx, row in control_data.iterrows():\n",
    "        trial_id = row[trial_id_col]\n",
    "        # Find intervention arm for this trial\n",
    "        intervention_data = df[(df[trial_id_col] == trial_id) & (df[control_arm_col] != 1)]\n",
    "        if not intervention_data.empty:\n",
    "            control_data.loc[idx, intervention_outcome_col] = intervention_data[target_col].mean()\n",
    "    \n",
    "    # Remove rows without intervention outcomes\n",
    "    control_data = control_data.dropna(subset=[intervention_outcome_col])\n",
    "    \n",
    "    # Use existing data preparation functions instead of manual standardization\n",
    "    print(f\"Raw data shape: {control_data.shape}\")\n",
    "    print(f\"Features to use: {features}\")\n",
    "    \n",
    "    # Prepare features using existing function (handles missing values properly)\n",
    "    feature_cols = features + [intervention_outcome_col, target_col]\n",
    "    prepared_data = prepare_data(control_data, feature_cols)\n",
    "    prepared_data = run_fillna(prepared_data)\n",
    "    \n",
    "    # Add back the trial ID for tracking\n",
    "    prepared_data[trial_id_col] = control_data[trial_id_col].values\n",
    "    \n",
    "    print(f\"Prepared data shape: {prepared_data.shape}\")\n",
    "    print(f\"Missing values after preparation: {prepared_data.isnull().sum().sum()}\")\n",
    "    \n",
    "    return prepared_data\n",
    "\n",
    "# Prepare data using existing preprocessing pipeline\n",
    "bayesian_data = prepare_bayesian_data(\n",
    "    training_df, bayesian_features, target_col, trial_id_col, control_arm_col\n",
    ")\n",
    "\n",
    "print(f\"\\nFinal Bayesian modeling dataset:\")\n",
    "print(f\"Shape: {bayesian_data.shape}\")\n",
    "print(f\"Columns: {list(bayesian_data.columns)}\")\n",
    "print(f\"Target variable stats: mean={bayesian_data[target_col].mean():.2f}, std={bayesian_data[target_col].std():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc8e8a88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['age_median',\n",
       " 'gender_male_percent',\n",
       " 'ecog_1',\n",
       " 'EGFR_positive_mutation',\n",
       " 'disease_stage_IV',\n",
       " 'PD1_PDL1_Inhibitor',\n",
       " 'EGFR_TKI',\n",
       " 'Platinum_Chemotherapy']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bayesian_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84f04b3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age_median</th>\n",
       "      <th>gender_male_percent</th>\n",
       "      <th>ecog_1</th>\n",
       "      <th>EGFR_positive_mutation</th>\n",
       "      <th>disease_stage_IV</th>\n",
       "      <th>PD1_PDL1_Inhibitor</th>\n",
       "      <th>EGFR_TKI</th>\n",
       "      <th>Platinum_Chemotherapy</th>\n",
       "      <th>intervention_outcome</th>\n",
       "      <th>PFS_median_months</th>\n",
       "      <th>NCT_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>56.000000</td>\n",
       "      <td>56.00</td>\n",
       "      <td>80.000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>91.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.20</td>\n",
       "      <td>6.5</td>\n",
       "      <td>NCT01364012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>62.000000</td>\n",
       "      <td>37.00</td>\n",
       "      <td>68.000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.80</td>\n",
       "      <td>10.9</td>\n",
       "      <td>NCT01469000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>64.000000</td>\n",
       "      <td>39.00</td>\n",
       "      <td>62.800</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.60</td>\n",
       "      <td>5.5</td>\n",
       "      <td>NCT03515837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>62.000000</td>\n",
       "      <td>34.00</td>\n",
       "      <td>58.000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.60</td>\n",
       "      <td>9.6</td>\n",
       "      <td>NCT04129502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>61.000000</td>\n",
       "      <td>59.50</td>\n",
       "      <td>79.700</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.50</td>\n",
       "      <td>7.1</td>\n",
       "      <td>NCT04194203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>62.000000</td>\n",
       "      <td>40.00</td>\n",
       "      <td>65.000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.40</td>\n",
       "      <td>6.7</td>\n",
       "      <td>NCT04538664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>62.000000</td>\n",
       "      <td>40.00</td>\n",
       "      <td>62.000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.30</td>\n",
       "      <td>4.2</td>\n",
       "      <td>NCT04988295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>66.000000</td>\n",
       "      <td>62.90</td>\n",
       "      <td>64.900</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.70</td>\n",
       "      <td>5.5</td>\n",
       "      <td>NCT02142738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>65.000000</td>\n",
       "      <td>55.00</td>\n",
       "      <td>64.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.20</td>\n",
       "      <td>5.9</td>\n",
       "      <td>NCT02041533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>63.000000</td>\n",
       "      <td>67.10</td>\n",
       "      <td>59.900</td>\n",
       "      <td>1.700000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.60</td>\n",
       "      <td>5.2</td>\n",
       "      <td>NCT02657434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>65.000000</td>\n",
       "      <td>59.00</td>\n",
       "      <td>60.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>91.366667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.00</td>\n",
       "      <td>5.5</td>\n",
       "      <td>NCT02367781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>62.894737</td>\n",
       "      <td>72.00</td>\n",
       "      <td>83.000</td>\n",
       "      <td>49.038462</td>\n",
       "      <td>91.366667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.00</td>\n",
       "      <td>6.5</td>\n",
       "      <td>NCT03134872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>62.000000</td>\n",
       "      <td>92.10</td>\n",
       "      <td>87.600</td>\n",
       "      <td>49.038462</td>\n",
       "      <td>75.300000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.50</td>\n",
       "      <td>4.9</td>\n",
       "      <td>NCT03629925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>66.000000</td>\n",
       "      <td>74.90</td>\n",
       "      <td>53.500</td>\n",
       "      <td>49.038462</td>\n",
       "      <td>86.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.10</td>\n",
       "      <td>8.1</td>\n",
       "      <td>NCT03117049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>61.000000</td>\n",
       "      <td>71.20</td>\n",
       "      <td>78.400</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>81.100000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.70</td>\n",
       "      <td>7.6</td>\n",
       "      <td>NCT03663205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>61.000000</td>\n",
       "      <td>59.50</td>\n",
       "      <td>79.700</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.50</td>\n",
       "      <td>7.1</td>\n",
       "      <td>NCT04194203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>66.000000</td>\n",
       "      <td>71.30</td>\n",
       "      <td>65.600</td>\n",
       "      <td>49.038462</td>\n",
       "      <td>91.366667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.60</td>\n",
       "      <td>4.2</td>\n",
       "      <td>NCT03829332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>62.894737</td>\n",
       "      <td>61.57</td>\n",
       "      <td>68.945</td>\n",
       "      <td>49.038462</td>\n",
       "      <td>91.366667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.10</td>\n",
       "      <td>9.5</td>\n",
       "      <td>NCT03829319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>62.000000</td>\n",
       "      <td>91.70</td>\n",
       "      <td>73.600</td>\n",
       "      <td>49.038462</td>\n",
       "      <td>63.600000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.60</td>\n",
       "      <td>5.5</td>\n",
       "      <td>NCT03594747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>64.000000</td>\n",
       "      <td>65.60</td>\n",
       "      <td>65.200</td>\n",
       "      <td>49.038462</td>\n",
       "      <td>91.366667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.65</td>\n",
       "      <td>5.6</td>\n",
       "      <td>NCT02477826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>62.894737</td>\n",
       "      <td>61.57</td>\n",
       "      <td>68.945</td>\n",
       "      <td>49.038462</td>\n",
       "      <td>91.366667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.60</td>\n",
       "      <td>4.2</td>\n",
       "      <td>NCT03866993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>65.000000</td>\n",
       "      <td>83.60</td>\n",
       "      <td>68.000</td>\n",
       "      <td>49.038462</td>\n",
       "      <td>91.366667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.80</td>\n",
       "      <td>4.8</td>\n",
       "      <td>NCT02775435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>62.894737</td>\n",
       "      <td>61.57</td>\n",
       "      <td>68.945</td>\n",
       "      <td>49.038462</td>\n",
       "      <td>91.366667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.80</td>\n",
       "      <td>5.2</td>\n",
       "      <td>NCT02775435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>62.894737</td>\n",
       "      <td>61.57</td>\n",
       "      <td>68.945</td>\n",
       "      <td>49.038462</td>\n",
       "      <td>91.366667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.00</td>\n",
       "      <td>5.8</td>\n",
       "      <td>NCT03728556</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    age_median  gender_male_percent  ecog_1  EGFR_positive_mutation  \\\n",
       "1    56.000000                56.00  80.000               26.000000   \n",
       "3    62.000000                37.00  68.000              100.000000   \n",
       "6    64.000000                39.00  62.800              100.000000   \n",
       "9    62.000000                34.00  58.000              100.000000   \n",
       "11   61.000000                59.50  79.700               54.000000   \n",
       "13   62.000000                40.00  65.000              100.000000   \n",
       "16   62.000000                40.00  62.000              100.000000   \n",
       "20   66.000000                62.90  64.900                0.000000   \n",
       "22   65.000000                55.00  64.000                0.000000   \n",
       "24   63.000000                67.10  59.900                1.700000   \n",
       "26   65.000000                59.00  60.000                0.000000   \n",
       "28   62.894737                72.00  83.000               49.038462   \n",
       "30   62.000000                92.10  87.600               49.038462   \n",
       "32   66.000000                74.90  53.500               49.038462   \n",
       "34   61.000000                71.20  78.400                1.800000   \n",
       "36   61.000000                59.50  79.700               54.000000   \n",
       "38   66.000000                71.30  65.600               49.038462   \n",
       "40   62.894737                61.57  68.945               49.038462   \n",
       "43   62.000000                91.70  73.600               49.038462   \n",
       "46   64.000000                65.60  65.200               49.038462   \n",
       "48   62.894737                61.57  68.945               49.038462   \n",
       "50   65.000000                83.60  68.000               49.038462   \n",
       "52   62.894737                61.57  68.945               49.038462   \n",
       "56   62.894737                61.57  68.945               49.038462   \n",
       "\n",
       "    disease_stage_IV  PD1_PDL1_Inhibitor  EGFR_TKI  Platinum_Chemotherapy  \\\n",
       "1          91.000000                 0.0       0.0                    1.0   \n",
       "3          88.000000                 0.0       1.0                    0.0   \n",
       "6         100.000000                 0.0       0.0                    1.0   \n",
       "9          95.000000                 0.0       0.0                    1.0   \n",
       "11        100.000000                 0.0       0.0                    1.0   \n",
       "13        100.000000                 0.0       0.0                    1.0   \n",
       "16        100.000000                 0.0       0.0                    1.0   \n",
       "20        100.000000                 0.0       0.0                    1.0   \n",
       "22         90.000000                 0.0       0.0                    1.0   \n",
       "24        100.000000                 0.0       0.0                    1.0   \n",
       "26         91.366667                 0.0       0.0                    1.0   \n",
       "28         91.366667                 0.0       0.0                    1.0   \n",
       "30         75.300000                 0.0       0.0                    1.0   \n",
       "32         86.500000                 0.0       0.0                    1.0   \n",
       "34         81.100000                 0.0       0.0                    1.0   \n",
       "36        100.000000                 0.0       0.0                    1.0   \n",
       "38         91.366667                 1.0       0.0                    0.0   \n",
       "40         91.366667                 1.0       0.0                    1.0   \n",
       "43         63.600000                 0.0       0.0                    1.0   \n",
       "46         91.366667                 0.0       0.0                    1.0   \n",
       "48         91.366667                 0.0       0.0                    1.0   \n",
       "50         91.366667                 0.0       0.0                    1.0   \n",
       "52         91.366667                 0.0       0.0                    1.0   \n",
       "56         91.366667                 0.0       0.0                    0.0   \n",
       "\n",
       "    intervention_outcome  PFS_median_months       NCT_ID  \n",
       "1                   9.20                6.5  NCT01364012  \n",
       "3                  15.80               10.9  NCT01469000  \n",
       "6                   5.60                5.5  NCT03515837  \n",
       "9                   9.60                9.6  NCT04129502  \n",
       "11                  9.50                7.1  NCT04194203  \n",
       "13                 11.40                6.7  NCT04538664  \n",
       "16                  7.30                4.2  NCT04988295  \n",
       "20                  7.70                5.5  NCT02142738  \n",
       "22                  4.20                5.9  NCT02041533  \n",
       "24                  7.60                5.2  NCT02657434  \n",
       "26                  7.00                5.5  NCT02367781  \n",
       "28                 11.00                6.5  NCT03134872  \n",
       "30                  5.50                4.9  NCT03629925  \n",
       "32                 12.10                8.1  NCT03117049  \n",
       "34                  9.70                7.6  NCT03663205  \n",
       "36                  9.50                7.1  NCT04194203  \n",
       "38                  6.60                4.2  NCT03829332  \n",
       "40                 12.10                9.5  NCT03829319  \n",
       "43                  7.60                5.5  NCT03594747  \n",
       "46                  4.65                5.6  NCT02477826  \n",
       "48                  7.60                4.2  NCT03866993  \n",
       "50                  6.80                4.8  NCT02775435  \n",
       "52                  6.80                5.2  NCT02775435  \n",
       "56                  9.00                5.8  NCT03728556  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bayesian_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f74fe187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building Bayesian control arm prediction model...\n",
      "Building model with 8 features and 24 observations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [intercept, beta_age_median, beta_gender_male_percent, beta_ecog_1, beta_EGFR_positive_mutation, beta_disease_stage_IV, beta_PD1_PDL1_Inhibitor, beta_EGFR_TKI, beta_Platinum_Chemotherapy, beta_intervention, sigma]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3341e04424b741e9b4d99b92d34647ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling 4 chains for 750 tune and 1_500 draw iterations (3_000 + 6_000 draws total) took 8 seconds.\n",
      "There were 1 divergences after tuning. Increase `target_accept` or reparameterize.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing log-likelihood...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2c5f19e33cb4792bef8598dc8bdaef7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Calculating model assessment metrics...\n",
      "WAIC: -39.64 ± 2.65\n",
      "LOO: -40.30 ± 2.76\n",
      "WAIC effective number of parameters: 5.80\n",
      "LOO effective number of parameters: 6.45\n",
      "\n",
      "Model summary:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/rct_opt_lean/lib/python3.12/site-packages/arviz/stats/stats.py:1667: UserWarning: For one or more samples the posterior variance of the log predictive densities exceeds 0.4. This could be indication of WAIC starting to fail. \n",
      "See http://arxiv.org/abs/1507.04544 for details\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/rct_opt_lean/lib/python3.12/site-packages/arviz/stats/stats.py:797: UserWarning: Estimated shape parameter of Pareto distribution is greater than 0.70 for one or more samples. You should consider using a more robust model, this is because importance sampling is less likely to work well if the marginal posterior and LOO posterior are very different. This is more likely to happen with a non-robust model and highly influential observations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>sd</th>\n",
       "      <th>hdi_2.5%</th>\n",
       "      <th>hdi_97.5%</th>\n",
       "      <th>mcse_mean</th>\n",
       "      <th>mcse_sd</th>\n",
       "      <th>ess_bulk</th>\n",
       "      <th>ess_tail</th>\n",
       "      <th>r_hat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>intercept</th>\n",
       "      <td>5.945</td>\n",
       "      <td>3.014</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>11.810</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.041</td>\n",
       "      <td>5054.0</td>\n",
       "      <td>4208.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beta_age_median</th>\n",
       "      <td>0.087</td>\n",
       "      <td>0.084</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>0.259</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.001</td>\n",
       "      <td>2121.0</td>\n",
       "      <td>2881.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beta_gender_male_percent</th>\n",
       "      <td>-0.060</td>\n",
       "      <td>0.030</td>\n",
       "      <td>-0.119</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2195.0</td>\n",
       "      <td>2992.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beta_ecog_1</th>\n",
       "      <td>0.015</td>\n",
       "      <td>0.031</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3467.0</td>\n",
       "      <td>3836.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beta_EGFR_positive_mutation</th>\n",
       "      <td>-0.008</td>\n",
       "      <td>0.009</td>\n",
       "      <td>-0.025</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4070.0</td>\n",
       "      <td>3683.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beta_disease_stage_IV</th>\n",
       "      <td>-0.073</td>\n",
       "      <td>0.041</td>\n",
       "      <td>-0.159</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>2522.0</td>\n",
       "      <td>3343.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beta_PD1_PDL1_Inhibitor</th>\n",
       "      <td>0.333</td>\n",
       "      <td>0.672</td>\n",
       "      <td>-0.927</td>\n",
       "      <td>1.668</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>5567.0</td>\n",
       "      <td>4005.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beta_EGFR_TKI</th>\n",
       "      <td>0.127</td>\n",
       "      <td>0.870</td>\n",
       "      <td>-1.598</td>\n",
       "      <td>1.792</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.011</td>\n",
       "      <td>5105.0</td>\n",
       "      <td>3961.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beta_Platinum_Chemotherapy</th>\n",
       "      <td>0.561</td>\n",
       "      <td>0.630</td>\n",
       "      <td>-0.734</td>\n",
       "      <td>1.740</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>5713.0</td>\n",
       "      <td>4498.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beta_intervention</th>\n",
       "      <td>0.491</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.299</td>\n",
       "      <td>0.690</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>5350.0</td>\n",
       "      <td>4085.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sigma</th>\n",
       "      <td>1.123</td>\n",
       "      <td>0.203</td>\n",
       "      <td>0.770</td>\n",
       "      <td>1.533</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>3868.0</td>\n",
       "      <td>3697.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              mean     sd  hdi_2.5%  hdi_97.5%  mcse_mean  \\\n",
       "intercept                    5.945  3.014    -0.046     11.810      0.042   \n",
       "beta_age_median              0.087  0.084    -0.070      0.259      0.002   \n",
       "beta_gender_male_percent    -0.060  0.030    -0.119      0.000      0.001   \n",
       "beta_ecog_1                  0.015  0.031    -0.048      0.075      0.001   \n",
       "beta_EGFR_positive_mutation -0.008  0.009    -0.025      0.010      0.000   \n",
       "beta_disease_stage_IV       -0.073  0.041    -0.159      0.004      0.001   \n",
       "beta_PD1_PDL1_Inhibitor      0.333  0.672    -0.927      1.668      0.009   \n",
       "beta_EGFR_TKI                0.127  0.870    -1.598      1.792      0.012   \n",
       "beta_Platinum_Chemotherapy   0.561  0.630    -0.734      1.740      0.008   \n",
       "beta_intervention            0.491  0.100     0.299      0.690      0.001   \n",
       "sigma                        1.123  0.203     0.770      1.533      0.003   \n",
       "\n",
       "                             mcse_sd  ess_bulk  ess_tail  r_hat  \n",
       "intercept                      0.041    5054.0    4208.0    1.0  \n",
       "beta_age_median                0.001    2121.0    2881.0    1.0  \n",
       "beta_gender_male_percent       0.000    2195.0    2992.0    1.0  \n",
       "beta_ecog_1                    0.000    3467.0    3836.0    1.0  \n",
       "beta_EGFR_positive_mutation    0.000    4070.0    3683.0    1.0  \n",
       "beta_disease_stage_IV          0.001    2522.0    3343.0    1.0  \n",
       "beta_PD1_PDL1_Inhibitor        0.009    5567.0    4005.0    1.0  \n",
       "beta_EGFR_TKI                  0.011    5105.0    3961.0    1.0  \n",
       "beta_Platinum_Chemotherapy     0.008    5713.0    4498.0    1.0  \n",
       "beta_intervention              0.001    5350.0    4085.0    1.0  \n",
       "sigma                          0.003    3868.0    3697.0    1.0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_bayesian_control_model(data, features, target_col, intervention_outcome_col):\n",
    "    \"\"\"Build Bayesian model with proper handling for mixed feature types and limited data\"\"\"\n",
    "    \n",
    "    print(f\"Building model with {len(features)} features and {len(data)} observations\")\n",
    "    \n",
    "    with pm.Model() as model:\n",
    "        # More conservative priors for limited data\n",
    "        intercept = pm.Normal('intercept', mu=6.0, sigma=3.0)  # Based on typical PFS values\n",
    "        \n",
    "        # Feature coefficients with conservative priors\n",
    "        beta_features = {}\n",
    "        for feature in features:\n",
    "            if feature in data.columns:\n",
    "                # Smaller prior variance for limited data to avoid overfitting\n",
    "                beta_features[feature] = pm.Normal(f'beta_{feature}', mu=0, sigma=1.0)\n",
    "        \n",
    "        # Intervention outcome coefficient (key for RCT Lean)\n",
    "        # Positive prior but conservative\n",
    "        beta_intervention = pm.Normal('beta_intervention', mu=0.3, sigma=0.5)\n",
    "        \n",
    "        # More conservative noise prior for limited data\n",
    "        sigma = pm.HalfNormal('sigma', sigma=2.0)\n",
    "        \n",
    "        # Linear combination - no standardization needed\n",
    "        mu = intercept + beta_intervention * data[intervention_outcome_col]\n",
    "        \n",
    "        for feature in features:\n",
    "            if feature in data.columns:\n",
    "                mu += beta_features[feature] * data[feature]\n",
    "        \n",
    "        # Likelihood\n",
    "        y_obs = pm.Normal('y_obs', mu=mu, sigma=sigma, observed=data[target_col])\n",
    "        \n",
    "        # More conservative sampling for limited data\n",
    "        trace = pm.sample(\n",
    "            draws=1500,        # Fewer draws\n",
    "            tune=750,          # Fewer tuning steps\n",
    "            # chains=2,          # Fewer chains\n",
    "            target_accept=0.9, # Higher target accept for stability\n",
    "            random_seed=42, \n",
    "            return_inferencedata=True,\n",
    "            progressbar=True,\n",
    "        )\n",
    "\n",
    "        print(\"Computing log-likelihood...\")\n",
    "        trace = pm.compute_log_likelihood(trace)\n",
    "        # Calculate model comparison metrics\n",
    "        print(\"\\nCalculating model assessment metrics...\")\n",
    "        # WAIC (Widely Applicable Information Criterion)\n",
    "        waic_data = az.waic(trace)\n",
    "        print(f\"WAIC: {waic_data.elpd_waic:.2f} ± {waic_data.se:.2f}\")\n",
    "        # LOO (Leave-One-Out Cross-Validation)\n",
    "        loo_data = az.loo(trace)\n",
    "        print(f\"LOO: {loo_data.elpd_loo:.2f} ± {loo_data.se:.2f}\")\n",
    "        # Print additional diagnostics\n",
    "        print(f\"WAIC effective number of parameters: {waic_data.p_waic:.2f}\")\n",
    "        print(f\"LOO effective number of parameters: {loo_data.p_loo:.2f}\")\n",
    "        pre_waic, pre_loo = waic_data.elpd_waic, loo_data.elpd_loo\n",
    "    \n",
    "    return model, trace\n",
    "\n",
    "# Build and fit the updated model\n",
    "print(\"\\nBuilding Bayesian control arm prediction model...\")\n",
    "bayesian_model, bayesian_trace = build_bayesian_control_model(\n",
    "    bayesian_data, bayesian_features, target_col, intervention_outcome_col\n",
    ")\n",
    "\n",
    "print(\"\\nModel summary:\")\n",
    "summary_df = az.summary(bayesian_trace, hdi_prob=0.95)\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4000bd51",
   "metadata": {},
   "source": [
    "# Find priors from dataset\n",
    "\n",
    "# Cheatsheet depending on which prior to choose for your dataset\n",
    "\n",
    "![Prior Table](../images/prior_bayesian.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32db09fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FEATURE ANALYSIS FOR PRIOR SELECTION:\n",
      "============================================================\n",
      "Feature Statistics:\n",
      "                         mean    std   min    max  range\n",
      "age_median              62.89   2.16  56.0   66.0   10.0\n",
      "gender_male_percent     61.57  15.80  34.0   92.1   58.1\n",
      "ecog_1                  68.94   8.60  53.5   87.6   34.1\n",
      "EGFR_positive_mutation  49.04  33.13   0.0  100.0  100.0\n",
      "disease_stage_IV        91.37   8.61  63.6  100.0   36.4\n",
      "PD1_PDL1_Inhibitor       0.08   0.28   0.0    1.0    1.0\n",
      "EGFR_TKI                 0.04   0.20   0.0    1.0    1.0\n",
      "Platinum_Chemotherapy    0.88   0.34   0.0    1.0    1.0\n",
      "\n",
      "Correlations with PFS_median_months:\n",
      "                  Feature  Correlation\n",
      "6                EGFR_TKI        0.552\n",
      "1     gender_male_percent       -0.374\n",
      "3  EGFR_positive_mutation        0.272\n",
      "0              age_median       -0.242\n",
      "7   Platinum_Chemotherapy       -0.142\n",
      "5      PD1_PDL1_Inhibitor        0.093\n",
      "2                  ecog_1       -0.035\n",
      "4        disease_stage_IV       -0.013\n",
      "\n",
      "PRIOR SELECTION ANALYSIS:\n",
      "========================================\n",
      "Target variable std: 1.77\n",
      "If a feature changes by 1 unit, how much should target change?\n",
      "\n",
      "Expected effect sizes (rule of thumb):\n",
      "- Small effect: ±0.35 months\n",
      "- Medium effect: ±0.88 months\n",
      "- Large effect: ±1.42 months\n",
      "\n",
      "STANDARDIZATION ANALYSIS:\n",
      "==============================\n",
      "✓ age_median: range = 10.0 (reasonable scale)\n",
      "⚠️  gender_male_percent: range = 58.1 (large scale)\n",
      "⚠️  ecog_1: range = 34.1 (large scale)\n",
      "⚠️  EGFR_positive_mutation: range = 100.0 (large scale)\n",
      "⚠️  disease_stage_IV: range = 36.4 (large scale)\n",
      "✓ PD1_PDL1_Inhibitor: range = 1.0 (reasonable scale)\n",
      "✓ EGFR_TKI: range = 1.0 (reasonable scale)\n",
      "✓ Platinum_Chemotherapy: range = 1.0 (reasonable scale)\n",
      "\n",
      "🚨 RECOMMENDATION: Standardize features before modeling!\n",
      "Reason: Large-scale features will dominate with current priors\n"
     ]
    }
   ],
   "source": [
    "def analyze_features_and_priors(data, features, target_col):\n",
    "    \"\"\"\n",
    "    Analyze feature scales and their relationship to target to set appropriate priors\n",
    "    \"\"\"\n",
    "    print(\"FEATURE ANALYSIS FOR PRIOR SELECTION:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    feature_stats = {}\n",
    "    \n",
    "    for feature in features:\n",
    "        if feature in data.columns:\n",
    "            values = data[feature]\n",
    "            feature_stats[feature] = {\n",
    "                'mean': values.mean(),\n",
    "                'std': values.std(),\n",
    "                'min': values.min(),\n",
    "                'max': values.max(),\n",
    "                'range': values.max() - values.min()\n",
    "            }\n",
    "    \n",
    "    # Display feature statistics\n",
    "    stats_df = pd.DataFrame(feature_stats).T\n",
    "    print(\"Feature Statistics:\")\n",
    "    print(stats_df.round(2))\n",
    "    \n",
    "    # Check correlation with target\n",
    "    correlations = {}\n",
    "    for feature in features:\n",
    "        if feature in data.columns:\n",
    "            corr = data[feature].corr(data[target_col])\n",
    "            correlations[feature] = corr\n",
    "    \n",
    "    corr_df = pd.DataFrame(list(correlations.items()), columns=['Feature', 'Correlation'])\n",
    "    corr_df = corr_df.sort_values('Correlation', key=abs, ascending=False)\n",
    "    \n",
    "    print(f\"\\nCorrelations with {target_col}:\")\n",
    "    print(corr_df.round(3))\n",
    "    \n",
    "    # PRIOR SELECTION REASONING\n",
    "    print(f\"\\nPRIOR SELECTION ANALYSIS:\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    target_std = data[target_col].std()\n",
    "    \n",
    "    print(f\"Target variable std: {target_std:.2f}\")\n",
    "    print(f\"If a feature changes by 1 unit, how much should target change?\")\n",
    "    \n",
    "    # Calculate effect sizes\n",
    "    print(f\"\\nExpected effect sizes (rule of thumb):\")\n",
    "    print(f\"- Small effect: ±{0.2 * target_std:.2f} months\")\n",
    "    print(f\"- Medium effect: ±{0.5 * target_std:.2f} months\") \n",
    "    print(f\"- Large effect: ±{0.8 * target_std:.2f} months\")\n",
    "    \n",
    "    # Check if standardization is needed\n",
    "    print(f\"\\nSTANDARDIZATION ANALYSIS:\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    need_standardization = False\n",
    "    for feature in features:\n",
    "        if feature in data.columns:\n",
    "            feature_range = stats_df.loc[feature, 'range']\n",
    "            if feature_range > 10:  # Arbitrary threshold\n",
    "                print(f\"⚠️  {feature}: range = {feature_range:.1f} (large scale)\")\n",
    "                need_standardization = True\n",
    "            else:\n",
    "                print(f\"✓ {feature}: range = {feature_range:.1f} (reasonable scale)\")\n",
    "    \n",
    "    if need_standardization:\n",
    "        print(f\"\\n🚨 RECOMMENDATION: Standardize features before modeling!\")\n",
    "        print(f\"Reason: Large-scale features will dominate with current priors\")\n",
    "    else:\n",
    "        print(f\"\\n✓ Feature scales are reasonable for current priors\")\n",
    "    \n",
    "    return stats_df, corr_df, need_standardization\n",
    "\n",
    "# Analyze your features\n",
    "feature_stats, correlations, needs_std = analyze_features_and_priors(\n",
    "    bayesian_data, bayesian_features, target_col\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c9acea",
   "metadata": {},
   "source": [
    "## Recommended Prior Distribution Strategy\n",
    "\n",
    "Based on our data analysis and clinical domain knowledge:\n",
    "\n",
    "### 1. **Normal Priors** - Our Current Choice ✅\n",
    "- **Continuous outcomes**: target outcome is continuous, centered around specific values. except outliers\n",
    "- **Limited data**: Normal priors provide regularization without being too restrictive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e51dc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add this cell to analyze your target variable and justify intercept prior\n",
    "def analyze_target_and_set_priors(data, target_col):\n",
    "    \"\"\"\n",
    "    Analyze target variable to set informed priors for intercept and coefficients\n",
    "    \"\"\"\n",
    "    target_values = data[target_col]\n",
    "    \n",
    "    print(\"TARGET VARIABLE ANALYSIS:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Target: {target_col}\")\n",
    "    print(f\"Mean: {target_values.mean():.2f}\")\n",
    "    print(f\"Median: {target_values.median():.2f}\")\n",
    "    print(f\"Std: {target_values.std():.2f}\")\n",
    "    print(f\"Min: {target_values.min():.2f}\")\n",
    "    print(f\"Max: {target_values.max():.2f}\")\n",
    "    print(f\"25th percentile: {target_values.quantile(0.25):.2f}\")\n",
    "    print(f\"75th percentile: {target_values.quantile(0.75):.2f}\")\n",
    "    \n",
    "    # Plot distribution\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    # Histogram\n",
    "    axes[0].hist(target_values, bins=20, alpha=0.7, density=True)\n",
    "    axes[0].axvline(target_values.mean(), color='red', linestyle='--', label=f'Mean: {target_values.mean():.2f}')\n",
    "    axes[0].axvline(target_values.median(), color='orange', linestyle='--', label=f'Median: {target_values.median():.2f}')\n",
    "    axes[0].set_xlabel(target_col)\n",
    "    axes[0].set_ylabel('Density')\n",
    "    axes[0].set_title('Target Variable Distribution')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Q-Q plot to check normality\n",
    "    from scipy import stats as scipy_stats\n",
    "    scipy_stats.probplot(target_values, dist=\"norm\", plot=axes[1])\n",
    "    axes[1].set_title('Q-Q Plot (Normal Distribution)')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # INTERCEPT PRIOR REASONING\n",
    "    print(\"\\nINTERCEPT PRIOR REASONING:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # When all features are at their mean values (approximately 0 after standardization),\n",
    "    # what should we expect the target to be?\n",
    "    target_mean = target_values.mean()\n",
    "    target_std = target_values.std()\n",
    "    \n",
    "    print(f\"When all features are at average levels:\")\n",
    "    print(f\"Expected outcome ≈ {target_mean:.2f} months (observed mean)\")\n",
    "    print(f\"Reasonable range: [{target_mean - 2*target_std:.2f}, {target_mean + 2*target_std:.2f}]\")\n",
    "    \n",
    "    # Clinical knowledge adjustment\n",
    "    print(f\"\\nClinical context for {target_col}:\")\n",
    "    if 'PFS' in target_col:\n",
    "        print(\"- PFS (Progression-Free Survival) typically ranges 2-20 months in oncology\")\n",
    "        print(\"- Most common range: 4-12 months\")\n",
    "        print(\"- Exceptional cases: up to 24+ months\")\n",
    "    \n",
    "    # Suggested prior\n",
    "    suggested_mu = target_mean\n",
    "    suggested_sigma = target_std  # Conservative: allows ±2 std deviation coverage\n",
    "    \n",
    "    print(f\"\\nSUGGESTED INTERCEPT PRIOR:\")\n",
    "    print(f\"pm.Normal('intercept', mu={suggested_mu:.1f}, sigma={suggested_sigma:.1f})\")\n",
    "    print(f\"This covers range: [{suggested_mu - 2*suggested_sigma:.1f}, {suggested_mu + 2*suggested_sigma:.1f}] with 95% probability\")\n",
    "    \n",
    "    return suggested_mu, suggested_sigma\n",
    "\n",
    "# Analyze your data\n",
    "intercept_mu, intercept_sigma = analyze_target_and_set_priors(bayesian_data, target_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bfce2a",
   "metadata": {},
   "source": [
    "## Train Bayesian model with prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93aa4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_bayesian_control_model_informed(data, features, target_col, intervention_outcome_col, \n",
    "                                        standardize=True, use_informed_priors=True):\n",
    "    \"\"\"\n",
    "    Build Bayesian model with mathematically informed priors and optional standardization\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"Building model with standardization={standardize}, informed_priors={use_informed_priors}\")\n",
    "    \n",
    "    # Prepare data\n",
    "    model_data = data.copy()\n",
    "    feature_transforms = {}\n",
    "    \n",
    "    if standardize:\n",
    "        print(\"Standardizing features...\")\n",
    "        for feature in features:\n",
    "            if feature in data.columns:\n",
    "                mean_val = data[feature].mean()\n",
    "                std_val = data[feature].std()\n",
    "                model_data[feature] = (data[feature] - mean_val) / std_val\n",
    "                feature_transforms[feature] = {'mean': mean_val, 'std': std_val}\n",
    "    \n",
    "    # Calculate informed priors\n",
    "    y_mean = data[target_col].mean()\n",
    "    y_std = data[target_col].std()\n",
    "    \n",
    "    if use_informed_priors:\n",
    "        # Informed priors based on analysis\n",
    "        if standardize:\n",
    "            intercept_mu = y_mean\n",
    "            intercept_sigma = y_std\n",
    "            coeff_sigma = 0.5 * y_std  # Conservative effect size\n",
    "        else:\n",
    "            intercept_mu = y_mean\n",
    "            intercept_sigma = 2 * y_std\n",
    "            coeff_sigma = 1.0  # Will be feature-specific in practice\n",
    "        \n",
    "        intervention_mu = 0.5\n",
    "        intervention_sigma = 0.3\n",
    "    else:\n",
    "        # Your original priors\n",
    "        intercept_mu = 6.0\n",
    "        intercept_sigma = 3.0\n",
    "        coeff_sigma = 1.0\n",
    "        intervention_mu = 0.3\n",
    "        intervention_sigma = 0.5\n",
    "    \n",
    "    print(f\"Using priors:\")\n",
    "    print(f\"  Intercept: Normal({intercept_mu:.1f}, {intercept_sigma:.1f})\")\n",
    "    print(f\"  Coefficients: Normal(0, {coeff_sigma:.1f})\")\n",
    "    print(f\"  Intervention: Normal({intervention_mu:.1f}, {intervention_sigma:.1f})\")\n",
    "    \n",
    "    with pm.Model() as model:\n",
    "        # Intercept\n",
    "        intercept = pm.Normal('intercept', mu=intercept_mu, sigma=intercept_sigma)\n",
    "        \n",
    "        # Feature coefficients\n",
    "        beta_features = {}\n",
    "        for feature in features:\n",
    "            if feature in model_data.columns:\n",
    "                beta_features[feature] = pm.Normal(f'beta_{feature}', mu=0, sigma=coeff_sigma)\n",
    "        \n",
    "        # Intervention coefficient\n",
    "        beta_intervention = pm.Normal('beta_intervention', mu=intervention_mu, sigma=intervention_sigma)\n",
    "        \n",
    "        # Noise\n",
    "        sigma = pm.HalfNormal('sigma', sigma=y_std)  # Informed noise prior\n",
    "        \n",
    "        # Linear combination\n",
    "        mu = intercept + beta_intervention * model_data[intervention_outcome_col]\n",
    "        for feature in features:\n",
    "            if feature in model_data.columns:\n",
    "                mu += beta_features[feature] * model_data[feature]\n",
    "        \n",
    "        # Likelihood\n",
    "        y_obs = pm.Normal('y_obs', mu=mu, sigma=sigma, observed=model_data[target_col])\n",
    "        \n",
    "        # Sample\n",
    "        trace = pm.sample(\n",
    "            draws=1500, tune=750, target_accept=0.9, \n",
    "            random_seed=42, return_inferencedata=True, progressbar=True\n",
    "        )\n",
    "        \n",
    "        # Model assessment\n",
    "        trace = pm.compute_log_likelihood(trace)\n",
    "        waic_data = az.waic(trace)\n",
    "        loo_data = az.loo(trace)\n",
    "        post_waic, post_loo = waic_data.elpd_waic, loo_data.elpd_loo\n",
    "        \n",
    "        print(f\"\\nModel Assessment:\")\n",
    "        print(f\"WAIC: {waic_data.elpd_waic:.2f} ± {waic_data.se:.2f}\")\n",
    "        print(f\"LOO: {loo_data.elpd_loo:.2f} ± {loo_data.se:.2f}\")\n",
    "    \n",
    "    return model, trace, feature_transforms\n",
    "\n",
    "# Compare models with different prior approaches\n",
    "print(\"Building model with standardization and informed priors...\")\n",
    "model_informed, trace_informed, transforms = build_bayesian_control_model_informed(\n",
    "    bayesian_data, bayesian_features, target_col, intervention_outcome_col,\n",
    "    standardize=True, use_informed_priors=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c860ff9",
   "metadata": {},
   "source": [
    "## After taking into account priors WAICC and LOO have improved slighlty\n",
    "\n",
    "```\n",
    "Prev WAIC -39.64 new -41.71, impact + 5.2%\n",
    "Prev LOO -40.30 new -43.44, impact + 7.8%\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2304b7e9",
   "metadata": {},
   "source": [
    "# Plot the Posterior Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220f62f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppc = pm.sample_posterior_predictive(trace_informed, model=model_informed)\n",
    "#ppc = pm.sample_posterior_predictive(bayesian_trace, model=bayesian_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e69ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18cdf61",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_samples = ppc.posterior_predictive['y_obs'].values.flatten()\n",
    "y_pred_mean = y_pred_samples.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad43e834",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(y_pred_samples, bins=50, alpha=0.7, density=True, label=\"Posterior\")\n",
    "\n",
    "plt.axvline(\n",
    "    y_pred_mean,\n",
    "    color=\"blue\",\n",
    "    linestyle=\"--\",\n",
    "    linewidth=2,\n",
    "    label=f\"ATE bayesian LR: {y_pred_mean:.2f}\",\n",
    ")\n",
    "plt.title(\"Posterior - Treatment Coefficient\")\n",
    "plt.xlabel(\"TE\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db613f78",
   "metadata": {},
   "source": [
    "# Inference on different population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8286091e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_bayesian(trace, new_data, n_samples=1000):\n",
    "    \"\"\"\n",
    "    For a given new_data dict (with keys: intervention_outcome and bayesian_features),\n",
    "    sample from the posterior to simulate a predictive distribution.\n",
    "    \"\"\"\n",
    "    posterior = trace.posterior\n",
    "    n_chains = posterior.dims['chain']\n",
    "    n_draws = posterior.dims['draw']\n",
    "    predictions = []\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        chain_idx = np.random.randint(0, n_chains)\n",
    "        draw_idx = np.random.randint(0, n_draws)\n",
    "        # Base linear predictor: intercept + beta_intervention * intervention_outcome\n",
    "        intercept_val = posterior['intercept'][chain_idx, draw_idx].values\n",
    "        beta_int_val = posterior['beta_intervention'][chain_idx, draw_idx].values\n",
    "        pred = intercept_val + beta_int_val * new_data[intervention_outcome_col]\n",
    "        # Add contribution from each feature\n",
    "        for f in bayesian_features:\n",
    "            beta_f = posterior[f'beta_{f}'][chain_idx, draw_idx].values\n",
    "            pred += beta_f * new_data[f]\n",
    "        # Add noise\n",
    "        sigma_val = posterior['sigma'][chain_idx, draw_idx].values\n",
    "        pred_sample = np.random.normal(pred, sigma_val)\n",
    "        predictions.append(pred_sample)\n",
    "    return np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b4c41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bayesian_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae1a3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "from utilities.bnn_utils import predict_bayesian_with_transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd2f95b",
   "metadata": {},
   "source": [
    "# Overall population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b626f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the columns we need to build a new data point:\n",
    "cols = [intervention_outcome_col] + bayesian_features\n",
    "\n",
    "# Overall new data point: take the mean (across the control arm data) for each column\n",
    "overall_new_data = bayesian_data[cols].mean().to_dict()\n",
    "overall_new_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5271d020",
   "metadata": {},
   "source": [
    "#  Posterior of Subpopulation (Age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6dc816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into young and old subpopulations based on the 'age_median' value\n",
    "median_age = bayesian_data['age_median'].median()\n",
    "young_new_data = bayesian_data[bayesian_data['age_median'] < median_age][cols].mean().to_dict()\n",
    "old_new_data   = bayesian_data[bayesian_data['age_median'] >= median_age][cols].mean().to_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588623c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "young_new_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be02c7d",
   "metadata": {},
   "source": [
    "# Option 1: Use original trace (no standardization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49aac81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the predictive distributions using the posterior trace (bayesian_trace_v2)\n",
    "overall_predictions = predict_bayesian(bayesian_trace, overall_new_data, n_samples=1000)\n",
    "young_predictions   = predict_bayesian(bayesian_trace, young_new_data, n_samples=1000)\n",
    "old_predictions     = predict_bayesian(bayesian_trace, old_new_data, n_samples=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ebbe967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display summary statistics\n",
    "print(\"Overall Predictions:\")\n",
    "print(f\"Mean: {np.mean(overall_predictions):.2f}, Std: {np.std(overall_predictions):.2f}\")\n",
    "print(f\"95% Credible Interval: [{np.percentile(overall_predictions, 2.5):.2f}, {np.percentile(overall_predictions, 97.5):.2f}]\")\n",
    "\n",
    "print(\"\\nYoung Subpopulation Predictions:\")\n",
    "print(f\"Mean: {np.mean(young_predictions):.2f}, Std: {np.std(young_predictions):.2f}\")\n",
    "print(f\"95% Credible Interval: [{np.percentile(young_predictions, 2.5):.2f}, {np.percentile(young_predictions, 97.5):.2f}]\")\n",
    "\n",
    "print(\"\\nOld Subpopulation Predictions:\")\n",
    "print(f\"Mean: {np.mean(old_predictions):.2f}, Std: {np.std(old_predictions):.2f}\")\n",
    "print(f\"95% Credible Interval: [{np.percentile(old_predictions, 2.5):.2f}, {np.percentile(old_predictions, 97.5):.2f}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7752f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "ax.hist(young_predictions, bins=50, alpha=0.6, label='Young Subpopulation', density=True, color='blue')\n",
    "ax.hist(old_predictions, bins=50, alpha=0.6, label='Old Subpopulation', density=True, color='red')\n",
    "\n",
    "ax.set_xlabel('Predicted Outcome')\n",
    "ax.set_ylabel('Density')\n",
    "ax.set_title('Posterior Predictive Distributions by Age Group')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8084608b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate means for each distribution\n",
    "overall_mean = np.mean(overall_predictions)\n",
    "young_mean = np.mean(young_predictions)\n",
    "old_mean = np.mean(old_predictions)\n",
    "\n",
    "# Visualization of posterior distributions\n",
    "fig, ax = plt.subplots(1, 3, figsize=(24, 8))\n",
    "\n",
    "ax[0].hist(overall_predictions, bins=50, density=True, alpha=0.7, color='skyblue')\n",
    "ax[0].axvline(overall_mean, color='darkblue', linestyle='--', linewidth=2, label=f'Mean: {overall_mean:.2f}')\n",
    "ax[0].set_title(f\"Overall Posterior Distribution (μ = {overall_mean:.2f})\")\n",
    "ax[0].set_xlabel(\"Predicted Outcome\")\n",
    "ax[0].set_ylabel(\"Density\")\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].hist(young_predictions, bins=50, density=True, alpha=0.7, color='green')\n",
    "ax[1].axvline(young_mean, color='darkgreen', linestyle='--', linewidth=2, label=f'Mean: {young_mean:.2f}')\n",
    "ax[1].set_title(f\"Young Subpopulation Posterior (μ = {young_mean:.2f})\")\n",
    "ax[1].set_xlabel(\"Predicted Outcome\")\n",
    "ax[1].legend()\n",
    "\n",
    "ax[2].hist(old_predictions, bins=50, density=True, alpha=0.7, color='red')\n",
    "ax[2].axvline(old_mean, color='darkred', linestyle='--', linewidth=2, label=f'Mean: {old_mean:.2f}')\n",
    "ax[2].set_title(f\"Old Subpopulation Posterior (μ = {old_mean:.2f})\")\n",
    "ax[2].set_xlabel(\"Predicted Outcome\")\n",
    "ax[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf9b1f1",
   "metadata": {},
   "source": [
    "# Option 2: Use informed trace (with standardization)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41957b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Using informed trace (with standardization):\")\n",
    "overall_predictions_informed = predict_bayesian_with_transforms(trace_informed, overall_new_data, transforms, n_samples=1000, intervention_outcome_col=intervention_outcome_col, bayesian_features=bayesian_features)\n",
    "young_predictions_informed = predict_bayesian_with_transforms(trace_informed, young_new_data, transforms, n_samples=1000, intervention_outcome_col=intervention_outcome_col, bayesian_features=bayesian_features)\n",
    "old_predictions_informed = predict_bayesian_with_transforms(trace_informed, old_new_data, transforms, n_samples=1000, intervention_outcome_col=intervention_outcome_col, bayesian_features=bayesian_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec34be0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nInformed model predictions:\")\n",
    "print(f\"Overall mean: {np.mean(overall_predictions_informed):.2f}\")\n",
    "print(f\"Young mean: {np.mean(young_predictions_informed):.2f}\")\n",
    "print(f\"Old mean: {np.mean(old_predictions_informed):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5699fff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "ax.hist(young_predictions_informed, bins=50, alpha=0.6, label='Young Subpopulation', density=True, color='blue')\n",
    "ax.hist(old_predictions_informed, bins=50, alpha=0.6, label='Old Subpopulation', density=True, color='red')\n",
    "\n",
    "ax.set_xlabel('Predicted Outcome')\n",
    "ax.set_ylabel('Density')\n",
    "ax.set_title('Posterior Predictive Distributions by Age Group')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efc1483",
   "metadata": {},
   "source": [
    "#  Posterior of Subpopulation (Disease Stage four)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0693a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new data for disease_stage_IV subpopulations: one for values < 90 and another for values ≥ 90\n",
    "low_dsiv_new_data = bayesian_data[bayesian_data['disease_stage_IV'] < 90][cols].mean().to_dict()\n",
    "high_dsiv_new_data = bayesian_data[bayesian_data['disease_stage_IV'] >= 90][cols].mean().to_dict()\n",
    "\n",
    "# Calculate the predictive distributions using the posterior trace (bayesian_trace_v2)\n",
    "low_dsiv_predictions = predict_bayesian(bayesian_trace, low_dsiv_new_data, n_samples=1000)\n",
    "high_dsiv_predictions = predict_bayesian(bayesian_trace, high_dsiv_new_data, n_samples=1000)\n",
    "\n",
    "# Plot the predictive distributions for the disease_stage_IV subpopulations\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "ax.hist(low_dsiv_predictions, bins=50, alpha=0.6, label='Disease Stage IV < 90', density=True, color='blue')\n",
    "ax.hist(high_dsiv_predictions, bins=50, alpha=0.6, label='Disease Stage IV ≥ 90', density=True, color='red')\n",
    "\n",
    "ax.set_xlabel('Predicted Outcome')\n",
    "ax.set_ylabel('Density')\n",
    "ax.set_title('Posterior Predictive Distributions by Disease Stage IV')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9397f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print summary statistics for disease_stage_IV subpopulation predictions\n",
    "print(\"\\nDisease Stage IV < 90 Predictions:\")\n",
    "print(f\"Mean: {np.mean(low_dsiv_predictions):.2f}, Std: {np.std(low_dsiv_predictions):.2f}\")\n",
    "print(f\"95% Credible Interval: [{np.percentile(low_dsiv_predictions, 2.5):.2f}, {np.percentile(low_dsiv_predictions, 97.5):.2f}]\")\n",
    "\n",
    "print(\"\\nDisease Stage IV ≥ 90 Predictions:\")\n",
    "print(f\"Mean: {np.mean(high_dsiv_predictions):.2f}, Std: {np.std(high_dsiv_predictions):.2f}\")\n",
    "print(f\"95% Credible Interval: [{np.percentile(high_dsiv_predictions, 2.5):.2f}, {np.percentile(high_dsiv_predictions, 97.5):.2f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31911f43",
   "metadata": {},
   "source": [
    "# Target ATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a318ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ate_probability(trace, subpop_data, intervention_outcome, target_ate, n_samples=1000):\n",
    "    \"\"\"\n",
    "    Calculate probability of achieving target ATE for a subpopulation\n",
    "    \n",
    "    Parameters:\n",
    "    - trace: Bayesian model trace\n",
    "    - subpop_data: Dictionary with subpopulation characteristics\n",
    "    - intervention_outcome: Expected intervention arm outcome\n",
    "    - target_ate: Target ATE value we want to achieve\n",
    "    - n_samples: Number of posterior samples\n",
    "    \n",
    "    Returns:\n",
    "    - prob_achieve: Probability of achieving target ATE\n",
    "    - ate_samples: Array of ATE samples for further analysis\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get control arm predictions for subpopulation\n",
    "    control_predictions = predict_bayesian(trace, subpop_data, n_samples=n_samples)\n",
    "    \n",
    "    # Calculate ATE samples: intervention - control\n",
    "    ate_samples = intervention_outcome - control_predictions\n",
    "    \n",
    "    # Calculate probability of achieving target ATE\n",
    "    prob_achieve = np.mean(ate_samples >= target_ate)\n",
    "    \n",
    "    return prob_achieve, ate_samples\n",
    "\n",
    "# Example usage with your existing data\n",
    "target_ate = 0.5\n",
    "\n",
    "# Calculate for young subpopulation\n",
    "prob_young, ate_young = calculate_ate_probability(\n",
    "    bayesian_trace, young_new_data, \n",
    "    young_new_data[intervention_outcome_col], target_ate\n",
    ")\n",
    "\n",
    "# Calculate for old subpopulation  \n",
    "prob_old, ate_old = calculate_ate_probability(\n",
    "    bayesian_trace, old_new_data,\n",
    "    old_new_data[intervention_outcome_col], target_ate\n",
    ")\n",
    "\n",
    "print(f\"Probability of ATE >= {target_ate}:\")\n",
    "print(f\"Young subpopulation: {prob_young:.3f}\")\n",
    "print(f\"Old subpopulation: {prob_old:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e1fb3f",
   "metadata": {},
   "source": [
    "# Bayesian Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cb5e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7a209b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities.bnn_utils import build_bnn_with_log_target, predict_log_transform_bnn, build_bayesian_neural_network, predict_bnn_with_transforms, compare_linear_vs_bnn_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecce2ad4",
   "metadata": {},
   "source": [
    "## Training on log of outcomes\n",
    "With one hidden layer and 25 no. of hidden neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6898e245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Log-transform approach\n",
    "print(\"\\n1. LOG-TRANSFORM APPROACH\")\n",
    "print(\"-\" * 40)\n",
    "model_log, trace_log, transforms_log = build_bnn_with_log_target(\n",
    "    bayesian_data, bayesian_features, target_col, intervention_outcome_col, hidden_units=[25]\n",
    ")\n",
    "preds_log = predict_log_transform_bnn(\n",
    "    trace_log, overall_new_data, transforms_log, bayesian_features, \n",
    "    intervention_outcome_col, [25], n_samples=1000\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"Mean: {np.mean(preds_log):.2f}, Std: {np.std(preds_log):.2f}\")\n",
    "print(f\"95% Credible Interval: [{np.percentile(preds_log, 2.5):.2f}, {np.percentile(preds_log, 97.5):.2f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12dd621",
   "metadata": {},
   "source": [
    "## Building a smaller netowrk\n",
    "Since amount of data is small, we reduced the network size and no. of hidden units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386a50f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnn_model, bnn_trace, bnn_transforms = build_bayesian_neural_network(\n",
    "        bayesian_data, bayesian_features, target_col, intervention_outcome_col,\n",
    "        hidden_units=[8],  # Smaller network for limited data\n",
    "        standardize=True, \n",
    "        use_informed_priors=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9b6861",
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9f4d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New data\n",
    "overall_comparison = compare_linear_vs_bnn_predictions(\n",
    "        trace_informed, bnn_trace, bnn_transforms, overall_new_data,\n",
    "        bayesian_features, intervention_outcome_col, transforms, hidden_units=[8]\n",
    ")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"BAYESIAN MODEL COMPARISON - OVERALL POPULATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Bayesian Linear Regression Results\n",
    "print(\"\\n📊 BAYESIAN LINEAR REGRESSION:\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Mean: {np.mean(overall_comparison['linear']):.2f} months\")\n",
    "print(f\"Std:  {np.std(overall_comparison['linear']):.2f} months\")\n",
    "print(f\"95% Credible Interval: [{np.percentile(overall_comparison['linear'], 2.5):.2f}, {np.percentile(overall_comparison['linear'], 97.5):.2f}] months\")\n",
    "print(f\"90% Credible Interval: [{np.percentile(overall_comparison['linear'], 5):.2f}, {np.percentile(overall_comparison['linear'], 95):.2f}] months\")\n",
    "\n",
    "# Bayesian Neural Network Results  \n",
    "print(\"\\n🧠 BAYESIAN NEURAL NETWORK:\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Mean: {np.mean(overall_comparison['bnn']):.2f} months\")\n",
    "print(f\"Std:  {np.std(overall_comparison['bnn']):.2f} months\")\n",
    "print(f\"95% Credible Interval: [{np.percentile(overall_comparison['bnn'], 2.5):.2f}, {np.percentile(overall_comparison['bnn'], 97.5):.2f}] months\")\n",
    "print(f\"90% Credible Interval: [{np.percentile(overall_comparison['bnn'], 5):.2f}, {np.percentile(overall_comparison['bnn'], 95):.2f}] months\")\n",
    "\n",
    "# Model Comparison\n",
    "print(\"\\n🔍 MODEL COMPARISON:\")\n",
    "print(\"-\" * 40)\n",
    "mean_diff = np.mean(overall_comparison['bnn']) - np.mean(overall_comparison['linear'])\n",
    "std_diff = np.std(overall_comparison['bnn']) - np.std(overall_comparison['linear'])\n",
    "print(f\"Mean difference (BNN - LR): {mean_diff:+.2f} months\")\n",
    "print(f\"Std difference (BNN - LR):  {std_diff:+.2f} months\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5588cf",
   "metadata": {},
   "source": [
    "## Compare  distributions of Bayesian NN vs Bayesian LR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b493f82c",
   "metadata": {},
   "source": [
    "### Young Subpopulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7f6ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Young population comparison\n",
    "young_comparison = compare_linear_vs_bnn_predictions(\n",
    "    trace_informed, bnn_trace, bnn_transforms, young_new_data,\n",
    "    bayesian_features, intervention_outcome_col, transforms, hidden_units=[8]\n",
    ")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"BAYESIAN MODEL COMPARISON - YOUNG SUBPOPULATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Bayesian Linear Regression Results\n",
    "print(\"\\n📊 BAYESIAN LINEAR REGRESSION (Young):\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Mean: {np.mean(young_comparison['linear']):.2f} months\")\n",
    "print(f\"Std:  {np.std(young_comparison['linear']):.2f} months\")\n",
    "print(f\"95% Credible Interval: [{np.percentile(young_comparison['linear'], 2.5):.2f}, {np.percentile(young_comparison['linear'], 97.5):.2f}] months\")\n",
    "print(f\"90% Credible Interval: [{np.percentile(young_comparison['linear'], 5):.2f}, {np.percentile(young_comparison['linear'], 95):.2f}] months\")\n",
    "\n",
    "# Bayesian Neural Network Results  \n",
    "print(\"\\n🧠 BAYESIAN NEURAL NETWORK (Young):\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Mean: {np.mean(young_comparison['bnn']):.2f} months\")\n",
    "print(f\"Std:  {np.std(young_comparison['bnn']):.2f} months\")\n",
    "print(f\"95% Credible Interval: [{np.percentile(young_comparison['bnn'], 2.5):.2f}, {np.percentile(young_comparison['bnn'], 97.5):.2f}] months\")\n",
    "print(f\"90% Credible Interval: [{np.percentile(young_comparison['bnn'], 5):.2f}, {np.percentile(young_comparison['bnn'], 95):.2f}] months\")\n",
    "\n",
    "# Model Comparison\n",
    "print(\"\\n🔍 MODEL COMPARISON (Young):\")\n",
    "print(\"-\" * 40)\n",
    "mean_diff_young = np.mean(young_comparison['bnn']) - np.mean(young_comparison['linear'])\n",
    "std_diff_young = np.std(young_comparison['bnn']) - np.std(young_comparison['linear'])\n",
    "print(f\"Mean difference (BNN - LR): {mean_diff_young:+.2f} months\")\n",
    "print(f\"Std difference (BNN - LR):  {std_diff_young:+.2f} months\")\n",
    "\n",
    "# Additional diagnostics\n",
    "print(f\"\\nNegative predictions (Young):\")\n",
    "print(f\"  Linear: {(young_comparison['linear'] < 0).mean():.1%}\")\n",
    "print(f\"  BNN:    {(young_comparison['bnn'] < 0).mean():.1%}\")\n",
    "\n",
    "print(f\"\\nPredictions > 12 months (Young):\")\n",
    "print(f\"  Linear: {(young_comparison['linear'] > 12).mean():.1%}\")\n",
    "print(f\"  BNN:    {(young_comparison['bnn'] > 12).mean():.1%}\")\n",
    "\n",
    "# Create separate plots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Plot 1: Overlapping distributions\n",
    "axes[0, 0].hist(young_comparison['linear'], bins=50, alpha=0.6, label='Bayesian Linear Regression', \n",
    "                density=True, color='blue')\n",
    "axes[0, 0].hist(young_comparison['bnn'], bins=50, alpha=0.6, label='Bayesian Neural Network', \n",
    "                density=True, color='red')\n",
    "axes[0, 0].axvline(np.mean(young_comparison['linear']), color='blue', linestyle='--', \n",
    "                   label=f'Linear Mean: {np.mean(young_comparison[\"linear\"]):.2f}')\n",
    "axes[0, 0].axvline(np.mean(young_comparison['bnn']), color='red', linestyle='--', \n",
    "                   label=f'BNN Mean: {np.mean(young_comparison[\"bnn\"]):.2f}')\n",
    "axes[0, 0].set_xlabel('Predicted PFS (months)')\n",
    "axes[0, 0].set_ylabel('Density')\n",
    "axes[0, 0].set_title('Young Subpopulation: Model Comparison')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Linear model only\n",
    "axes[0, 1].hist(young_comparison['linear'], bins=50, alpha=0.7, density=True, color='blue')\n",
    "axes[0, 1].axvline(np.mean(young_comparison['linear']), color='darkblue', linestyle='--', linewidth=2, \n",
    "                   label=f'Mean: {np.mean(young_comparison[\"linear\"]):.2f}')\n",
    "axes[0, 1].set_xlabel('Predicted PFS (months)')\n",
    "axes[0, 1].set_ylabel('Density')\n",
    "axes[0, 1].set_title('Young Subpopulation: Bayesian Linear Regression')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: BNN model only\n",
    "axes[1, 0].hist(young_comparison['bnn'], bins=50, alpha=0.7, density=True, color='red')\n",
    "axes[1, 0].axvline(np.mean(young_comparison['bnn']), color='darkred', linestyle='--', linewidth=2, \n",
    "                   label=f'Mean: {np.mean(young_comparison[\"bnn\"]):.2f}')\n",
    "axes[1, 0].set_xlabel('Predicted PFS (months)')\n",
    "axes[1, 0].set_ylabel('Density')\n",
    "axes[1, 0].set_title('Young Subpopulation: Bayesian Neural Network')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Box plot comparison\n",
    "box_data = [young_comparison['linear'], young_comparison['bnn']]\n",
    "box_labels = ['Linear Regression', 'Neural Network']\n",
    "box_plot = axes[1, 1].boxplot(box_data, labels=box_labels, patch_artist=True)\n",
    "\n",
    "# Color the boxes\n",
    "colors = ['lightblue', 'lightcoral']\n",
    "for patch, color in zip(box_plot['boxes'], colors):\n",
    "    patch.set_facecolor(color)\n",
    "\n",
    "axes[1, 1].set_ylabel('Predicted PFS (months)')\n",
    "axes[1, 1].set_title('Young Subpopulation: Box Plot Comparison')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary table\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"YOUNG SUBPOPULATION SUMMARY TABLE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Model':<20} {'Mean':<8} {'Std':<8} {'95% CI':<20} {'Neg %':<8} {'>12mo %':<8}\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'Linear Regression':<20} {np.mean(young_comparison['linear']):<8.2f} {np.std(young_comparison['linear']):<8.2f} [{np.percentile(young_comparison['linear'], 2.5):.2f}, {np.percentile(young_comparison['linear'], 97.5):.2f}]     {(young_comparison['linear'] < 0).mean()*100:<8.1f} {(young_comparison['linear'] > 12).mean()*100:<8.1f}\")\n",
    "print(f\"{'Neural Network':<20} {np.mean(young_comparison['bnn']):<8.2f} {np.std(young_comparison['bnn']):<8.2f} [{np.percentile(young_comparison['bnn'], 2.5):.2f}, {np.percentile(young_comparison['bnn'], 97.5):.2f}]     {(young_comparison['bnn'] < 0).mean()*100:<8.1f} {(young_comparison['bnn'] > 12).mean()*100:<8.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b760686",
   "metadata": {},
   "source": [
    "#### Old population - Bayesian LR vs Bayesian NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f371b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Old population comparison\n",
    "old_comparison = compare_linear_vs_bnn_predictions(\n",
    "    trace_informed, bnn_trace, bnn_transforms, old_new_data,\n",
    "    bayesian_features, intervention_outcome_col, transforms, hidden_units=[8]\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BAYESIAN MODEL COMPARISON - OLD SUBPOPULATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Print similar statistics for old population\n",
    "print(\"\\n📊 BAYESIAN LINEAR REGRESSION (Old):\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Mean: {np.mean(old_comparison['linear']):.2f} months\")\n",
    "print(f\"Std:  {np.std(old_comparison['linear']):.2f} months\")\n",
    "print(f\"95% Credible Interval: [{np.percentile(old_comparison['linear'], 2.5):.2f}, {np.percentile(old_comparison['linear'], 97.5):.2f}] months\")\n",
    "\n",
    "print(\"\\n🧠 BAYESIAN NEURAL NETWORK (Old):\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Mean: {np.mean(old_comparison['bnn']):.2f} months\")\n",
    "print(f\"Std:  {np.std(old_comparison['bnn']):.2f} months\")\n",
    "print(f\"95% Credible Interval: [{np.percentile(old_comparison['bnn'], 2.5):.2f}, {np.percentile(old_comparison['bnn'], 97.5):.2f}] months\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce6b361",
   "metadata": {},
   "source": [
    "### Bayesian NN - Young vs Old Populations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8491a779",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Plot 1: Young vs Old - Linear Regression\n",
    "axes[0, 0].hist(young_comparison['linear'], bins=40, alpha=0.6, label='Young Subpopulation', \n",
    "                density=True, color='blue')\n",
    "axes[0, 0].hist(old_comparison['linear'], bins=40, alpha=0.6, label='Old Subpopulation', \n",
    "                density=True, color='orange')\n",
    "axes[0, 0].axvline(np.mean(young_comparison['linear']), color='blue', linestyle='--', linewidth=2,\n",
    "                   label=f'Young Mean: {np.mean(young_comparison[\"linear\"]):.2f}')\n",
    "axes[0, 0].axvline(np.mean(old_comparison['linear']), color='orange', linestyle='--', linewidth=2,\n",
    "                   label=f'Old Mean: {np.mean(old_comparison[\"linear\"]):.2f}')\n",
    "axes[0, 0].set_xlabel('Predicted PFS (months)')\n",
    "axes[0, 0].set_ylabel('Density')\n",
    "axes[0, 0].set_title('📊 Bayesian Linear Regression: Young vs Old')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Young vs Old - Bayesian Neural Network\n",
    "axes[0, 1].hist(young_comparison['bnn'], bins=40, alpha=0.6, label='Young Subpopulation', \n",
    "                density=True, color='green')\n",
    "axes[0, 1].hist(old_comparison['bnn'], bins=40, alpha=0.6, label='Old Subpopulation', \n",
    "                density=True, color='red')\n",
    "axes[0, 1].axvline(np.mean(young_comparison['bnn']), color='green', linestyle='--', linewidth=2,\n",
    "                   label=f'Young Mean: {np.mean(young_comparison[\"bnn\"]):.2f}')\n",
    "axes[0, 1].axvline(np.mean(old_comparison['bnn']), color='red', linestyle='--', linewidth=2,\n",
    "                   label=f'Old Mean: {np.mean(old_comparison[\"bnn\"]):.2f}')\n",
    "axes[0, 1].set_xlabel('Predicted PFS (months)')\n",
    "axes[0, 1].set_ylabel('Density')\n",
    "axes[0, 1].set_title('🧠 Bayesian Neural Network: Young vs Old')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Young Population - Model Comparison\n",
    "axes[1, 0].hist(young_comparison['linear'], bins=35, alpha=0.6, label='Linear Regression', \n",
    "                density=True, color='blue')\n",
    "axes[1, 0].hist(young_comparison['bnn'], bins=35, alpha=0.6, label='Neural Network', \n",
    "                density=True, color='green')\n",
    "axes[1, 0].axvline(np.mean(young_comparison['linear']), color='blue', linestyle='--', linewidth=2,\n",
    "                   label=f'LR Mean: {np.mean(young_comparison[\"linear\"]):.2f}')\n",
    "axes[1, 0].axvline(np.mean(young_comparison['bnn']), color='green', linestyle='--', linewidth=2,\n",
    "                   label=f'BNN Mean: {np.mean(young_comparison[\"bnn\"]):.2f}')\n",
    "axes[1, 0].set_xlabel('Predicted PFS (months)')\n",
    "axes[1, 0].set_ylabel('Density')\n",
    "axes[1, 0].set_title('Young Subpopulation: LR vs BNN')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Old Population - Model Comparison\n",
    "axes[1, 1].hist(old_comparison['linear'], bins=35, alpha=0.6, label='Linear Regression', \n",
    "                density=True, color='orange')\n",
    "axes[1, 1].hist(old_comparison['bnn'], bins=35, alpha=0.6, label='Neural Network', \n",
    "                density=True, color='red')\n",
    "axes[1, 1].axvline(np.mean(old_comparison['linear']), color='orange', linestyle='--', linewidth=2,\n",
    "                   label=f'LR Mean: {np.mean(old_comparison[\"linear\"]):.2f}')\n",
    "axes[1, 1].axvline(np.mean(old_comparison['bnn']), color='red', linestyle='--', linewidth=2,\n",
    "                   label=f'BNN Mean: {np.mean(old_comparison[\"bnn\"]):.2f}')\n",
    "axes[1, 1].set_xlabel('Predicted PFS (months)')\n",
    "axes[1, 1].set_ylabel('Density')\n",
    "axes[1, 1].set_title('Old Subpopulation: LR vs BNN')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aecfd3c0",
   "metadata": {},
   "source": [
    "# Plot Posterior Distribution - BNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d024f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bnn_posterior_predictive():\n",
    "    \"\"\"\n",
    "    Plot posterior predictive distribution for BNN model\n",
    "    \"\"\"\n",
    "    \n",
    "    ppc_bnn = pm.sample_posterior_predictive(bnn_trace, model=bnn_model)\n",
    "    y_pred_samples = ppc_bnn.posterior_predictive['y_obs'].values.flatten()\n",
    "    y_pred_mean = y_pred_samples.mean()\n",
    "    plt.hist(y_pred_samples, bins=50, alpha=0.7, density=True, label=\"Posterior\")\n",
    "\n",
    "    plt.axvline(\n",
    "        y_pred_mean,\n",
    "        color=\"blue\",\n",
    "        linestyle=\"--\",\n",
    "        linewidth=2,\n",
    "        label=f\"ATE bayesian BNN: {y_pred_mean:.2f}\",\n",
    "    )\n",
    "    plt.title(\"Posterior - Treatment Coefficient\")\n",
    "    plt.xlabel(\"TE\")\n",
    "    plt.ylabel(\"Density\")\n",
    "    plt.legend()\n",
    "\n",
    "plot_bnn_posterior_predictive()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44812de9",
   "metadata": {},
   "source": [
    "# Gaussian Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9bca5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bcdbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities.gp_utils import build_gaussian_process, predict_gp, plot_gp_predictions, compute_kernel_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af2faad",
   "metadata": {},
   "outputs": [],
   "source": [
    "gp_model, gp_trace, gp_params = build_gaussian_process(\n",
    "    bayesian_data,\n",
    "    bayesian_features,\n",
    "    target_col,\n",
    "    intervention_outcome_col,\n",
    "    kernel='matern52',\n",
    "    standardize=True,\n",
    "    use_log_target=True  # Set to True to ensure positive predictions\n",
    ")\n",
    "\n",
    "# Make predictions\n",
    "young_preds = predict_gp(\n",
    "    gp_trace, gp_params, young_new_data,\n",
    "    bayesian_features, intervention_outcome_col,\n",
    "    n_samples=2000, return_std=False\n",
    ")\n",
    "\n",
    "print(f\"Young Population GP Predictions:\")\n",
    "print(f\"Mean: {np.mean(young_preds):.2f}, Std: {np.std(young_preds):.2f}\")\n",
    "print(f\"95% CI: [{np.percentile(young_preds, 2.5):.2f}, {np.percentile(young_preds, 97.5):.2f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7910b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gp_predictions(young_predictions, title=\"Young Population - GP Predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7b3dfd",
   "metadata": {},
   "source": [
    "# Point Estimatation\n",
    "Leave-one out protocol to get R2, RMSE values and compare with other tree based models\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51e8375",
   "metadata": {},
   "outputs": [],
   "source": [
    "bayesian_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b31e3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_ftr_selected = bayesian_features.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f92479",
   "metadata": {},
   "source": [
    "### Inference  using vectorized operations\n",
    "\n",
    "We could use ```predict_bayesian``` function as an alternative if dataset is small. But for the sake of demonstration of approach we use ```predict_vectorized```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c34e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_vectorized(trace, new_data_scaled, bayesian_features, n_samples=1000):\n",
    "    \"\"\"\n",
    "    Vectorized prediction - returns 1D array using dynamic Bayesian features.\n",
    "    \n",
    "    For each feature in bayesian_features, the corresponding parameter in the posterior\n",
    "    is assumed to be beta_{feature}.\n",
    "    \"\"\"\n",
    "    posterior_samples = trace.posterior\n",
    "\n",
    "    # Randomly sample indices from the posterior\n",
    "    n_chains, n_draws = posterior_samples.dims['chain'], posterior_samples.dims['draw']\n",
    "    chain_indices = np.random.randint(0, n_chains, n_samples)\n",
    "    draw_indices = np.random.randint(0, n_draws, n_samples)\n",
    "\n",
    "    # Extract the intercept and sigma samples\n",
    "    intercept_samples = posterior_samples['intercept'][chain_indices, draw_indices].values.flatten()\n",
    "    sigma_samples = posterior_samples['sigma'][chain_indices, draw_indices].values.flatten()\n",
    "\n",
    "    # Initialize the linear predictor with the intercept\n",
    "    mu_pred = intercept_samples.copy()\n",
    "    \n",
    "    # Add contribution from each feature dynamically\n",
    "    for feature in bayesian_features:\n",
    "        param_name = f\"beta_{feature}\"\n",
    "        beta_samples = posterior_samples[param_name][chain_indices, draw_indices].values.flatten()\n",
    "        mu_pred += beta_samples * new_data_scaled[feature]\n",
    "    \n",
    "    # Add noise for the final prediction\n",
    "    predictions = np.random.normal(mu_pred, sigma_samples)\n",
    "    \n",
    "    return predictions  # Returns a 1D numpy array of shape (n_samples,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aee7db9",
   "metadata": {},
   "source": [
    "# Run leave-one-out Bayesian inference for control arm prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36637490",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_bayesian_model_fast(data, features, target_col, intervention_outcome_col, \n",
    "                             draws=500, tune=250):\n",
    "    \"\"\"\n",
    "    Build and sample Bayesian model with custom sampling parameters for leave-one-out inference.\n",
    "    Returns only the trace for prediction.\n",
    "    \"\"\"\n",
    "    \n",
    "    with pm.Model() as model:\n",
    "        # Same priors as original build_bayesian_control_model\n",
    "        intercept = pm.Normal('intercept', mu=0, sigma=10.0)\n",
    "        \n",
    "        # Feature coefficients\n",
    "        beta_features = {}\n",
    "        for feature in features:\n",
    "            if feature in data.columns:\n",
    "                beta_features[feature] = pm.Normal(f'beta_{feature}', mu=0, sigma=1.0)\n",
    "        \n",
    "        # Intervention coefficient\n",
    "        beta_intervention = pm.Normal('beta_intervention', mu=0.3, sigma=0.5)\n",
    "        \n",
    "        # Noise parameter\n",
    "        sigma = pm.HalfNormal('sigma', sigma=2.0)\n",
    "        \n",
    "        # Linear combination\n",
    "        mu = intercept + beta_intervention * data[intervention_outcome_col]\n",
    "        for feature in features:\n",
    "            if feature in data.columns:\n",
    "                mu += beta_features[feature] * data[feature]\n",
    "        \n",
    "        # Likelihood\n",
    "        y_obs = pm.Normal('y_obs', mu=mu, sigma=sigma, observed=data[target_col])\n",
    "        \n",
    "        # Sample with custom parameters\n",
    "        trace = pm.sample(\n",
    "            draws=draws, tune=tune, target_accept=0.9, \n",
    "            random_seed=42, return_inferencedata=True, progressbar=False, verbose=False, \n",
    "        )\n",
    "    \n",
    "    return trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b166ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hide all pymc related logs\n",
    "\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "logger = logging.getLogger(\"pymc\")\n",
    "logger.setLevel(logging.ERROR)\n",
    "# suppress all FutureWarning messages\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893ca71d",
   "metadata": {},
   "source": [
    "## Bayesian LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52839fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with pred CI and actual CI for OUTCOME and ATE\n",
    "\n",
    "results_bayesian = []\n",
    "bayesian_draws = 500\n",
    "bayesian_tune = 250\n",
    "\n",
    "for index, row in training_df.iterrows():\n",
    "    rct_name = row[trial_id_col]\n",
    "    is_arm_control = row[control_arm_col]\n",
    "    \n",
    "    if is_arm_control == 1:\n",
    "        # Ground truth\n",
    "        outcome_control = round(row[target_col], 2)\n",
    "        \n",
    "        # ==========================================\n",
    "        # COLLECT ACTUAL CI DATA FOR IoU CALCULATION\n",
    "        # ==========================================\n",
    "        \n",
    "        # Get the actual CI for the current control arm (for IoU calculation)\n",
    "        actual_outcome_ci = None\n",
    "        if \"PFS_median_CI\" in training_df.columns:\n",
    "            actual_ci_value = row[\"PFS_median_CI\"]\n",
    "            if pd.notna(actual_ci_value) and actual_ci_value is not None:\n",
    "                # Handle different formats of CI storage\n",
    "                if isinstance(actual_ci_value, (list, tuple, np.ndarray)):\n",
    "                    actual_outcome_ci = actual_ci_value\n",
    "                elif isinstance(actual_ci_value, str):\n",
    "                    try:\n",
    "                        # Try to parse string representation like \"[x, y]\"\n",
    "                        actual_outcome_ci = eval(actual_ci_value)\n",
    "                    except:\n",
    "                        print(f\"Warning: Could not parse CI string for {rct_name}: {actual_ci_value}\")\n",
    "                        actual_outcome_ci = None\n",
    "                else:\n",
    "                    actual_outcome_ci = None\n",
    "        \n",
    "        # Get the trt outcome of the RCT targeted\n",
    "        trt_arm = training_df.loc[training_df[trial_id_col] == rct_name, :]\n",
    "        trt_outcome = trt_arm.loc[trt_arm[control_arm_col] != 1, target_col]\n",
    "        if trt_outcome.empty or pd.isna(trt_outcome.mean()):\n",
    "            continue\n",
    "        trt_outcome = round(trt_outcome.mean(), 2)\n",
    "        \n",
    "        # Prepare training data\n",
    "        training_j = training_df.copy()\n",
    "        # Flag inference row\n",
    "        training_j[\"is_to_predict\"] = np.where(training_j.index == index, 1, 0)\n",
    "        # Flag RCT row\n",
    "        training_j[\"is_targeted_rct\"] = np.where(training_j[trial_id_col] == rct_name, 1, 0)\n",
    "        \n",
    "        # Add intervention outcome as a feature for all control arms\n",
    "        for idx, control_row in training_j[training_j[control_arm_col] == 1].iterrows():\n",
    "            ctrl_rct = control_row[trial_id_col]\n",
    "            # Get intervention outcome for this control's trial\n",
    "            ctrl_trt_outcome = training_j.loc[(training_j[trial_id_col] == ctrl_rct) & \n",
    "                                             (training_j[control_arm_col] != 1), target_col]\n",
    "            \n",
    "            if not ctrl_trt_outcome.empty and not pd.isna(ctrl_trt_outcome.mean()):\n",
    "                training_j.loc[idx, intervention_outcome_col] = round(ctrl_trt_outcome.mean(), 2)\n",
    "            else:\n",
    "                training_j.loc[idx, intervention_outcome_col] = np.nan\n",
    "        \n",
    "        training_j = training_j.dropna(subset=[intervention_outcome_col])\n",
    "        \n",
    "        # Only consider control arms for training\n",
    "        training_j = training_j.loc[training_j[control_arm_col] == 1, :]\n",
    "        training_j = prepare_data(training_j, list_ftr_selected + [\"is_to_predict\", \"is_targeted_rct\", intervention_outcome_col, target_col])\n",
    "        training_j = run_fillna(training_j)\n",
    "        \n",
    "        # Make sure the target row has the intervention outcome\n",
    "        training_j.loc[training_j[\"is_to_predict\"] == 1, intervention_outcome_col] = trt_outcome\n",
    "        \n",
    "        # Extract inference data\n",
    "        inference_df = training_j.loc[training_j[\"is_to_predict\"] == 1, :]\n",
    "        if inference_df.empty:\n",
    "            print(f\"Skipping {rct_name} as no inference data available.\")\n",
    "            continue\n",
    "            \n",
    "        # Remove inference row and targeted RCT from training\n",
    "        training_subset = training_j.loc[training_j[\"is_to_predict\"] != 1, :]\n",
    "        training_subset = training_subset.loc[training_subset[\"is_targeted_rct\"] != 1, :]\n",
    "        \n",
    "        # Check if we have enough training data\n",
    "        if training_subset.shape[0] < 10:\n",
    "            print(f\"Skipping {rct_name} due to insufficient data for Bayesian inference.\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            # ==========================================\n",
    "            # CALCULATE TRAINING ACCURACY METRICS\n",
    "            # ==========================================\n",
    "            \n",
    "            # Build and sample Bayesian model\n",
    "            bayesian_trace = build_bayesian_model_fast(\n",
    "                training_subset, list_ftr_selected, target_col, intervention_outcome_col,\n",
    "                draws=bayesian_draws, tune=bayesian_tune\n",
    "            )\n",
    "            \n",
    "            # Get training predictions for evaluation\n",
    "            training_preds = []\n",
    "            for _, train_row in training_subset.iterrows():\n",
    "                train_data = train_row[list_ftr_selected + [intervention_outcome_col]].to_dict()\n",
    "                train_pred_samples = predict_bayesian(bayesian_trace, train_data, n_samples=500)\n",
    "                training_preds.append(np.mean(train_pred_samples))\n",
    "            \n",
    "            training_preds = np.array(training_preds)\n",
    "            training_true = training_subset[target_col].values\n",
    "            \n",
    "            # Calculate training metrics\n",
    "            training_rmse = round(np.sqrt(mean_squared_error(training_true, training_preds)), 3)\n",
    "            training_r2 = round(r2_score(training_true, training_preds), 3)\n",
    "            training_mae = round(np.mean(np.abs(training_true - training_preds)), 3)\n",
    "            \n",
    "            print(f\"  Training metrics - RMSE: {training_rmse}, R²: {training_r2}, MAE: {training_mae}\")\n",
    "            \n",
    "            # ==========================================\n",
    "            # GET TEST PREDICTION WITH UNCERTAINTY\n",
    "            # ==========================================\n",
    "            \n",
    "            # Create prediction data\n",
    "            new_data = inference_df[list_ftr_selected + [intervention_outcome_col]].iloc[0].to_dict()\n",
    "            \n",
    "            # Make predictions with full posterior distribution\n",
    "            pred_samples = predict_bayesian(bayesian_trace, new_data, n_samples=2000)\n",
    "            \n",
    "            # Calculate prediction statistics\n",
    "            predicted_outcome = round(np.nanmean(pred_samples), 2)\n",
    "            pred_std = round(np.nanstd(pred_samples), 2)\n",
    "            pred_median = round(np.nanmedian(pred_samples), 2)\n",
    "            \n",
    "            # Calculate confidence intervals\n",
    "            pred_ci_95 = [\n",
    "                round(np.percentile(pred_samples, 2.5), 2),\n",
    "                round(np.percentile(pred_samples, 97.5), 2)\n",
    "            ]\n",
    "            pred_ci_90 = [\n",
    "                round(np.percentile(pred_samples, 5.0), 2),\n",
    "                round(np.percentile(pred_samples, 95.0), 2)\n",
    "            ]\n",
    "            pred_ci_80 = [\n",
    "                round(np.percentile(pred_samples, 10.0), 2),\n",
    "                round(np.percentile(pred_samples, 90.0), 2)\n",
    "            ]\n",
    "            \n",
    "            # Additional uncertainty measures\n",
    "            pred_ci_width = round(pred_ci_95[1] - pred_ci_95[0], 3)\n",
    "            pred_iqr = round(np.percentile(pred_samples, 75) - np.percentile(pred_samples, 25), 3)\n",
    "            \n",
    "            # ==========================================\n",
    "            # CALCULATE ATE DISTRIBUTION\n",
    "            # ==========================================\n",
    "            \n",
    "            # Calculate ATEs\n",
    "            real_ate = round(trt_outcome - outcome_control, 2)\n",
    "            pred_ate = round(trt_outcome - predicted_outcome, 2)\n",
    "            \n",
    "            # Calculate ATE distribution (treatment outcome - predicted control distribution)\n",
    "            ate_samples = trt_outcome - pred_samples\n",
    "            ate_mean = round(np.mean(ate_samples), 2)\n",
    "            ate_std = round(np.std(ate_samples), 2)\n",
    "            ate_ci_95 = [\n",
    "                round(np.percentile(ate_samples, 2.5), 2),\n",
    "                round(np.percentile(ate_samples, 97.5), 2)\n",
    "            ]\n",
    "            ate_ci_90 = [\n",
    "                round(np.percentile(ate_samples, 5.0), 2),\n",
    "                round(np.percentile(ate_samples, 95.0), 2)\n",
    "            ]\n",
    "            ate_ci_width = round(ate_ci_95[1] - ate_ci_95[0], 3)\n",
    "            \n",
    "            # Probability of positive ATE\n",
    "            prob_positive_ate = round(np.mean(ate_samples > 0), 3)\n",
    "            \n",
    "            # Empirical-based ATE metrics (alternative calculation)\n",
    "            ate_ci_95_empirical = [\n",
    "                round(trt_outcome - pred_ci_95[1], 2),  # Note: reversed order\n",
    "                round(trt_outcome - pred_ci_95[0], 2)\n",
    "            ]\n",
    "            ate_ci_90_empirical = [\n",
    "                round(trt_outcome - pred_ci_90[1], 2),\n",
    "                round(trt_outcome - pred_ci_90[0], 2)\n",
    "            ]\n",
    "            \n",
    "            # Alternative probability calculation using empirical quantiles\n",
    "            if pred_ci_95[1] < trt_outcome:\n",
    "                prob_positive_ate_empirical = 1.0\n",
    "            elif pred_ci_95[0] > trt_outcome:\n",
    "                prob_positive_ate_empirical = 0.0\n",
    "            else:\n",
    "                prob_positive_ate_empirical = prob_positive_ate\n",
    "            \n",
    "            # ==========================================\n",
    "            # STORE ENHANCED RESULTS WITH ACTUAL CI\n",
    "            # ==========================================\n",
    "            \n",
    "            results_bayesian.append({\n",
    "                # Original results\n",
    "                \"real_ate\": real_ate,\n",
    "                \"pred_ate\": pred_ate,\n",
    "                \"outcome_control\": outcome_control,\n",
    "                \"predicted_outcome\": predicted_outcome,\n",
    "                \"rct_name\": rct_name,\n",
    "                \"intervention\": row[\"intervention\"],\n",
    "                \"Arm\": row[\"Arm\"],\n",
    "                \"n_training_samples\": len(training_subset),\n",
    "                \n",
    "                # ATE distribution metrics (needed for performance evaluation)\n",
    "                \"ate_mean\": ate_mean,\n",
    "                \"ate_std\": ate_std,\n",
    "                \"ate_ci_95\": ate_ci_95,\n",
    "                \"ate_ci_90\": ate_ci_90,\n",
    "                \"ate_ci_width\": ate_ci_width,\n",
    "                \"ate_samples\": ate_samples.tolist(),\n",
    "                \"prob_positive_ate\": prob_positive_ate,\n",
    "                \n",
    "                # Enhanced prediction uncertainty metrics\n",
    "                \"pred_std\": pred_std,\n",
    "                \"pred_median\": pred_median,\n",
    "                \"pred_ci_95\": pred_ci_95,\n",
    "                \"pred_ci_90\": pred_ci_90,\n",
    "                \"pred_ci_80\": pred_ci_80,\n",
    "                \"pred_ci_width\": pred_ci_width,\n",
    "                \"pred_iqr\": pred_iqr,\n",
    "                \"pred_samples\": pred_samples.tolist(),\n",
    "                \n",
    "                # Empirical-based ATE metrics (more accurate)\n",
    "                \"ate_ci_95_empirical\": ate_ci_95_empirical,\n",
    "                \"ate_ci_90_empirical\": ate_ci_90_empirical,\n",
    "                \"prob_positive_ate_empirical\": prob_positive_ate_empirical,\n",
    "                \n",
    "                # Training accuracy metrics\n",
    "                \"training_rmse\": training_rmse,\n",
    "                \"training_r2\": training_r2,\n",
    "                \"training_mae\": training_mae,\n",
    "                \n",
    "                # Additional training set statistics\n",
    "                \"training_set_std\": round(np.std(training_true), 3),\n",
    "                \"training_pred_std\": round(np.std(training_preds), 3),\n",
    "                \n",
    "                # NEW: Data for IoU calculation\n",
    "                \"actual_outcome_ci\": actual_outcome_ci,  # Actual CI from RCT data\n",
    "                \"original_row_index\": index,  # Store original index for potential merging\n",
    "                \n",
    "                # Additional uncertainty measures for comprehensive evaluation\n",
    "                \"pred_range_90\": round(pred_ci_90[1] - pred_ci_90[0], 3),\n",
    "                \"pred_range_80\": round(pred_ci_80[1] - pred_ci_80[0], 3),\n",
    "            })\n",
    "            \n",
    "            print(f\"{rct_name} - arm: {row['Arm']} - intervention: {trt_outcome}\")\n",
    "            print(f\"  Real outcome: {outcome_control}, Pred: {predicted_outcome} ± {pred_std}\")\n",
    "            print(f\"  Pred CI 95%: {pred_ci_95}, width: {pred_ci_width}\")\n",
    "            if actual_outcome_ci:\n",
    "                print(f\"  Actual CI: {actual_outcome_ci}\")\n",
    "            print(f\"  Real ATE: {real_ate}, Pred ATE: {pred_ate} (95% CI: {ate_ci_95})\")\n",
    "            print(f\"  P(ATE > 0): {prob_positive_ate} (empirical: {prob_positive_ate_empirical})\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error with {rct_name}: {str(e)}\")\n",
    "            continue\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8b5de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_bayesian_df = pd.DataFrame(results_bayesian)\n",
    "results_bayesian_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051e44ac",
   "metadata": {},
   "source": [
    "## Bayesian Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf3a98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytensor.tensor as pt\n",
    "\n",
    "def build_bnn_model_fast_loo(data, features, target_col, intervention_outcome_col, \n",
    "                            hidden_units=[8], draws=500, tune=250):\n",
    "    \"\"\"\n",
    "    Build and sample BNN model with custom sampling parameters for leave-one-out inference.\n",
    "    Returns the trace and feature transforms for prediction.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Standardize features for BNN\n",
    "    model_data = data.copy()\n",
    "    feature_transforms = {}\n",
    "    \n",
    "    # Standardize input features\n",
    "    for feature in features:\n",
    "        if feature in data.columns:\n",
    "            mean_val = data[feature].mean()\n",
    "            std_val = data[feature].std()\n",
    "            if std_val > 0:\n",
    "                model_data[feature] = (data[feature] - mean_val) / std_val\n",
    "            else:\n",
    "                model_data[feature] = data[feature] - mean_val\n",
    "            feature_transforms[feature] = {\n",
    "                \"mean\": mean_val,\n",
    "                \"std\": std_val if std_val > 0 else 1.0,\n",
    "            }\n",
    "    \n",
    "    # Standardize intervention outcome\n",
    "    int_mean = data[intervention_outcome_col].mean()\n",
    "    int_std = data[intervention_outcome_col].std()\n",
    "    if int_std > 0:\n",
    "        model_data[intervention_outcome_col] = (data[intervention_outcome_col] - int_mean) / int_std\n",
    "    else:\n",
    "        model_data[intervention_outcome_col] = data[intervention_outcome_col] - int_mean\n",
    "    feature_transforms[intervention_outcome_col] = {\n",
    "        \"mean\": int_mean,\n",
    "        \"std\": int_std if int_std > 0 else 1.0,\n",
    "    }\n",
    "    \n",
    "    # Standardize target variable\n",
    "    target_mean = data[target_col].mean()\n",
    "    target_std = data[target_col].std()\n",
    "    if target_std > 0:\n",
    "        model_data[target_col] = (data[target_col] - target_mean) / target_std\n",
    "    else:\n",
    "        model_data[target_col] = data[target_col] - target_mean\n",
    "    feature_transforms[target_col] = {\n",
    "        \"mean\": target_mean,\n",
    "        \"std\": target_std if target_std > 0 else 1.0,\n",
    "    }\n",
    "    \n",
    "    # Prepare input features\n",
    "    X_features = model_data[features].values\n",
    "    X_intervention = model_data[intervention_outcome_col].values.reshape(-1, 1)\n",
    "    X_combined = np.concatenate([X_features, X_intervention], axis=1)\n",
    "    y = model_data[target_col].values\n",
    "    \n",
    "    n_samples, n_input = X_combined.shape\n",
    "    \n",
    "    with pm.Model() as model:\n",
    "        X_tensor = pt.as_tensor_variable(X_combined)\n",
    "        layer_sizes = [n_input] + hidden_units + [1]\n",
    "        current_input = X_tensor\n",
    "        \n",
    "        # Conservative priors for limited data\n",
    "        weight_sigma = 0.5 / np.sqrt(n_input)\n",
    "        bias_sigma = 0.1\n",
    "        \n",
    "        for i, (in_size, out_size) in enumerate(zip(layer_sizes[:-1], layer_sizes[1:])):\n",
    "            W = pm.Normal(f\"W_{i}\", mu=0, sigma=weight_sigma, shape=(in_size, out_size))\n",
    "            b = pm.Normal(f\"b_{i}\", mu=0, sigma=bias_sigma, shape=out_size)\n",
    "            \n",
    "            linear_out = pt.dot(current_input, W) + b\n",
    "            \n",
    "            if i < len(layer_sizes) - 2:  # Hidden layers\n",
    "                current_input = pt.maximum(linear_out, 0)  # ReLU activation\n",
    "            else:  # Output layer\n",
    "                network_output = linear_out.flatten()\n",
    "        \n",
    "        # Noise parameter\n",
    "        sigma = pm.HalfNormal(\"sigma\", sigma=0.5)\n",
    "        \n",
    "        # Likelihood\n",
    "        y_obs = pm.Normal(\"y_obs\", mu=network_output, sigma=sigma, observed=y)\n",
    "        \n",
    "        # Sample with custom parameters\n",
    "        trace = pm.sample(\n",
    "            draws=draws, tune=tune, target_accept=0.9, \n",
    "            random_seed=42, return_inferencedata=True, \n",
    "            progressbar=False, verbose=False, chains=2\n",
    "        )\n",
    "    \n",
    "    return trace, feature_transforms\n",
    "\n",
    "def predict_bnn_fast_loo(trace, new_data, feature_transforms, features, \n",
    "                        intervention_outcome_col, hidden_units=[8], n_samples=1000):\n",
    "    \"\"\"\n",
    "    Fast BNN prediction for leave-one-out evaluation\n",
    "    \"\"\"\n",
    "    # Apply standardization\n",
    "    standardized_data = new_data.copy()\n",
    "    for feature in features + [intervention_outcome_col]:\n",
    "        if feature in standardized_data and feature in feature_transforms:\n",
    "            transform = feature_transforms[feature]\n",
    "            mean_val = transform[\"mean\"]\n",
    "            std_val = transform[\"std\"]\n",
    "            standardized_data[feature] = (standardized_data[feature] - mean_val) / std_val\n",
    "    \n",
    "    # Prepare input vector\n",
    "    X_features = np.array([standardized_data[f] for f in features])\n",
    "    X_intervention = np.array([standardized_data[intervention_outcome_col]])\n",
    "    X_combined = np.concatenate([X_features, X_intervention])\n",
    "    \n",
    "    posterior = trace.posterior\n",
    "    n_chains = posterior.dims[\"chain\"]\n",
    "    n_draws = posterior.dims[\"draw\"]\n",
    "    predictions = []\n",
    "    \n",
    "    layer_sizes = [len(features) + 1] + hidden_units + [1]\n",
    "    \n",
    "    for _ in range(n_samples):\n",
    "        chain_idx = np.random.randint(0, n_chains)\n",
    "        draw_idx = np.random.randint(0, n_draws)\n",
    "        \n",
    "        current_input = X_combined\n",
    "        \n",
    "        for layer_idx, (in_size, out_size) in enumerate(zip(layer_sizes[:-1], layer_sizes[1:])):\n",
    "            W = posterior[f\"W_{layer_idx}\"][chain_idx, draw_idx].values\n",
    "            b = posterior[f\"b_{layer_idx}\"][chain_idx, draw_idx].values\n",
    "            \n",
    "            linear_out = np.dot(current_input, W) + b\n",
    "            \n",
    "            if layer_idx < len(layer_sizes) - 2:\n",
    "                current_input = np.maximum(linear_out, 0)  # ReLU\n",
    "            else:\n",
    "                network_output = linear_out[0]\n",
    "        \n",
    "        sigma_val = posterior[\"sigma\"][chain_idx, draw_idx].values\n",
    "        pred_sample = np.random.normal(network_output, sigma_val)\n",
    "        predictions.append(pred_sample)\n",
    "    \n",
    "    predictions = np.array(predictions)\n",
    "    \n",
    "    # Transform back to original scale\n",
    "    target_transform = feature_transforms[target_col]\n",
    "    target_mean = target_transform[\"mean\"]\n",
    "    target_std = target_transform[\"std\"]\n",
    "    predictions = predictions * target_std + target_mean\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5830c122",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_bayesian_bnn = []\n",
    "bayesian_draws = 500\n",
    "bayesian_tune = 250\n",
    "\n",
    "for index, row in training_df.iterrows():\n",
    "    rct_name = row[trial_id_col]\n",
    "    is_arm_control = row[control_arm_col]\n",
    "    \n",
    "    if is_arm_control == 1:\n",
    "        print(f\"\\nProcessing {rct_name}...\")\n",
    "        \n",
    "        # Ground truth\n",
    "        outcome_control = round(row[target_col], 2)\n",
    "        \n",
    "        # ==========================================\n",
    "        # COLLECT ACTUAL CI DATA FOR IoU CALCULATION\n",
    "        # ==========================================\n",
    "        \n",
    "        # Get the actual CI for the current control arm (for IoU calculation)\n",
    "        actual_outcome_ci = None\n",
    "        if \"PFS_median_CI\" in training_df.columns:\n",
    "            actual_ci_value = row[\"PFS_median_CI\"]\n",
    "            if pd.notna(actual_ci_value) and actual_ci_value is not None:\n",
    "                # Handle different formats of CI storage\n",
    "                if isinstance(actual_ci_value, (list, tuple, np.ndarray)):\n",
    "                    actual_outcome_ci = actual_ci_value\n",
    "                elif isinstance(actual_ci_value, str):\n",
    "                    try:\n",
    "                        # Try to parse string representation like \"[x, y]\"\n",
    "                        actual_outcome_ci = eval(actual_ci_value)\n",
    "                    except:\n",
    "                        print(f\"Warning: Could not parse CI string for {rct_name}: {actual_ci_value}\")\n",
    "                        actual_outcome_ci = None\n",
    "                else:\n",
    "                    actual_outcome_ci = None\n",
    "        \n",
    "        # Get the trt outcome of the RCT targeted\n",
    "        trt_arm = training_df.loc[training_df[trial_id_col] == rct_name, :]\n",
    "        trt_outcome = trt_arm.loc[trt_arm[control_arm_col] != 1, target_col]\n",
    "        if trt_outcome.empty or pd.isna(trt_outcome.mean()):\n",
    "            continue\n",
    "        trt_outcome = round(trt_outcome.mean(), 2)\n",
    "        \n",
    "        # Prepare training data\n",
    "        training_j = training_df.copy()\n",
    "        # Flag inference row\n",
    "        training_j[\"is_to_predict\"] = np.where(training_j.index == index, 1, 0)\n",
    "        # Flag RCT row\n",
    "        training_j[\"is_targeted_rct\"] = np.where(training_j[trial_id_col] == rct_name, 1, 0)\n",
    "        \n",
    "        # Add intervention outcome as a feature for all control arms\n",
    "        for idx, control_row in training_j[training_j[control_arm_col] == 1].iterrows():\n",
    "            ctrl_rct = control_row[trial_id_col]\n",
    "            # Get intervention outcome for this control's trial\n",
    "            ctrl_trt_outcome = training_j.loc[(training_j[trial_id_col] == ctrl_rct) & \n",
    "                                             (training_j[control_arm_col] != 1), target_col]\n",
    "            \n",
    "            if not ctrl_trt_outcome.empty and not pd.isna(ctrl_trt_outcome.mean()):\n",
    "                training_j.loc[idx, intervention_outcome_col] = round(ctrl_trt_outcome.mean(), 2)\n",
    "            else:\n",
    "                training_j.loc[idx, intervention_outcome_col] = np.nan\n",
    "        \n",
    "        training_j = training_j.dropna(subset=[intervention_outcome_col])\n",
    "        \n",
    "        # Only consider control arms for training\n",
    "        training_j = training_j.loc[training_j[control_arm_col] == 1, :]\n",
    "        training_j = prepare_data(training_j, list_ftr_selected + [\"is_to_predict\", \"is_targeted_rct\", intervention_outcome_col, target_col])\n",
    "        training_j = run_fillna(training_j)\n",
    "        \n",
    "        # Make sure the target row has the intervention outcome\n",
    "        training_j.loc[training_j[\"is_to_predict\"] == 1, intervention_outcome_col] = trt_outcome\n",
    "        \n",
    "        # Extract inference data\n",
    "        inference_df = training_j.loc[training_j[\"is_to_predict\"] == 1, :]\n",
    "        if inference_df.empty:\n",
    "            print(f\"Skipping {rct_name} as no inference data available.\")\n",
    "            continue\n",
    "            \n",
    "        # Remove inference row and targeted RCT from training\n",
    "        training_subset = training_j.loc[training_j[\"is_to_predict\"] != 1, :]\n",
    "        training_subset = training_subset.loc[training_subset[\"is_targeted_rct\"] != 1, :]\n",
    "        \n",
    "        # Check if we have enough training data\n",
    "        if training_subset.shape[0] < 10:\n",
    "            print(f\"Skipping {rct_name} due to insufficient data for Bayesian inference.\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            # ==========================================\n",
    "            # BUILD BNN AND CALCULATE TRAINING ACCURACY METRICS\n",
    "            # ==========================================\n",
    "            \n",
    "            print(f\"  Building BNN model...\")\n",
    "            # Build and sample BNN model\n",
    "            bnn_trace, bnn_transforms = build_bnn_model_fast_loo(\n",
    "                training_subset, list_ftr_selected, target_col, intervention_outcome_col,\n",
    "                hidden_units=[8], draws=bayesian_draws, tune=bayesian_tune\n",
    "            )\n",
    "            \n",
    "            # Get BNN training predictions for evaluation\n",
    "            bnn_training_preds = []\n",
    "            for _, train_row in training_subset.iterrows():\n",
    "                train_data = train_row[list_ftr_selected + [intervention_outcome_col]].to_dict()\n",
    "                train_pred_samples = predict_bnn_fast_loo(\n",
    "                    bnn_trace, train_data, bnn_transforms, list_ftr_selected, \n",
    "                    intervention_outcome_col, hidden_units=[8], n_samples=500\n",
    "                )\n",
    "                bnn_training_preds.append(np.mean(train_pred_samples))\n",
    "            \n",
    "            bnn_training_preds = np.array(bnn_training_preds)\n",
    "            training_true = training_subset[target_col].values\n",
    "            \n",
    "            # Calculate BNN training metrics\n",
    "            bnn_training_rmse = round(np.sqrt(mean_squared_error(training_true, bnn_training_preds)), 3)\n",
    "            bnn_training_r2 = round(r2_score(training_true, bnn_training_preds), 3)\n",
    "            bnn_training_mae = round(np.mean(np.abs(training_true - bnn_training_preds)), 3)\n",
    "            \n",
    "            print(f\"  BNN Training metrics - RMSE: {bnn_training_rmse}, R²: {bnn_training_r2}, MAE: {bnn_training_mae}\")\n",
    "            \n",
    "            # ==========================================\n",
    "            # GET BNN TEST PREDICTION WITH UNCERTAINTY\n",
    "            # ==========================================\n",
    "            \n",
    "            # Create prediction data\n",
    "            new_data = inference_df[list_ftr_selected + [intervention_outcome_col]].iloc[0].to_dict()\n",
    "            \n",
    "            # Make BNN predictions with full posterior distribution\n",
    "            bnn_pred_samples = predict_bnn_fast_loo(\n",
    "                bnn_trace, new_data, bnn_transforms, list_ftr_selected, \n",
    "                intervention_outcome_col, hidden_units=[8], n_samples=2000\n",
    "            )\n",
    "            \n",
    "            # Calculate BNN prediction statistics\n",
    "            bnn_predicted_outcome = round(np.nanmean(bnn_pred_samples), 2)\n",
    "            bnn_pred_std = round(np.nanstd(bnn_pred_samples), 2)\n",
    "            bnn_pred_median = round(np.nanmedian(bnn_pred_samples), 2)\n",
    "            \n",
    "            # Calculate BNN confidence intervals\n",
    "            bnn_pred_ci_95 = [\n",
    "                round(np.percentile(bnn_pred_samples, 2.5), 2),\n",
    "                round(np.percentile(bnn_pred_samples, 97.5), 2)\n",
    "            ]\n",
    "            bnn_pred_ci_90 = [\n",
    "                round(np.percentile(bnn_pred_samples, 5.0), 2),\n",
    "                round(np.percentile(bnn_pred_samples, 95.0), 2)\n",
    "            ]\n",
    "            bnn_pred_ci_80 = [\n",
    "                round(np.percentile(bnn_pred_samples, 10.0), 2),\n",
    "                round(np.percentile(bnn_pred_samples, 90.0), 2)\n",
    "            ]\n",
    "            \n",
    "            # Additional BNN uncertainty measures\n",
    "            bnn_pred_ci_width = round(bnn_pred_ci_95[1] - bnn_pred_ci_95[0], 3)\n",
    "            bnn_pred_iqr = round(np.percentile(bnn_pred_samples, 75) - np.percentile(bnn_pred_samples, 25), 3)\n",
    "            \n",
    "            # ==========================================\n",
    "            # CALCULATE BNN ATE DISTRIBUTION\n",
    "            # ==========================================\n",
    "            \n",
    "            # Calculate BNN ATEs\n",
    "            real_ate = round(trt_outcome - outcome_control, 2)\n",
    "            bnn_pred_ate = round(trt_outcome - bnn_predicted_outcome, 2)\n",
    "            \n",
    "            # Calculate BNN ATE distribution (treatment outcome - predicted control distribution)\n",
    "            bnn_ate_samples = trt_outcome - bnn_pred_samples\n",
    "            bnn_ate_mean = round(np.mean(bnn_ate_samples), 2)\n",
    "            bnn_ate_std = round(np.std(bnn_ate_samples), 2)\n",
    "            bnn_ate_ci_95 = [\n",
    "                round(np.percentile(bnn_ate_samples, 2.5), 2),\n",
    "                round(np.percentile(bnn_ate_samples, 97.5), 2)\n",
    "            ]\n",
    "            bnn_ate_ci_90 = [\n",
    "                round(np.percentile(bnn_ate_samples, 5.0), 2),\n",
    "                round(np.percentile(bnn_ate_samples, 95.0), 2)\n",
    "            ]\n",
    "            bnn_ate_ci_width = round(bnn_ate_ci_95[1] - bnn_ate_ci_95[0], 3)\n",
    "            \n",
    "            # BNN Probability of positive ATE\n",
    "            bnn_prob_positive_ate = round(np.mean(bnn_ate_samples > 0), 3)\n",
    "            \n",
    "            # BNN Empirical-based ATE metrics (alternative calculation)\n",
    "            bnn_ate_ci_95_empirical = [\n",
    "                round(trt_outcome - bnn_pred_ci_95[1], 2),  # Note: reversed order\n",
    "                round(trt_outcome - bnn_pred_ci_95[0], 2)\n",
    "            ]\n",
    "            bnn_ate_ci_90_empirical = [\n",
    "                round(trt_outcome - bnn_pred_ci_90[1], 2),\n",
    "                round(trt_outcome - bnn_pred_ci_90[0], 2)\n",
    "            ]\n",
    "            \n",
    "            # Alternative BNN probability calculation using empirical quantiles\n",
    "            if bnn_pred_ci_95[1] < trt_outcome:\n",
    "                bnn_prob_positive_ate_empirical = 1.0\n",
    "            elif bnn_pred_ci_95[0] > trt_outcome:\n",
    "                bnn_prob_positive_ate_empirical = 0.0\n",
    "            else:\n",
    "                bnn_prob_positive_ate_empirical = bnn_prob_positive_ate\n",
    "            \n",
    "            # ==========================================\n",
    "            # STORE BNN RESULTS WITH ACTUAL CI\n",
    "            # ==========================================\n",
    "            \n",
    "            results_bayesian_bnn.append({\n",
    "                # Original results\n",
    "                \"real_ate\": real_ate,\n",
    "                \"pred_ate\": bnn_pred_ate,\n",
    "                \"outcome_control\": outcome_control,\n",
    "                \"predicted_outcome\": bnn_predicted_outcome,\n",
    "                \"rct_name\": rct_name,\n",
    "                \"intervention\": row[\"intervention\"],\n",
    "                \"Arm\": row[\"Arm\"],\n",
    "                \"n_training_samples\": len(training_subset),\n",
    "                \n",
    "                # BNN ATE distribution metrics (needed for performance evaluation)\n",
    "                \"ate_mean\": bnn_ate_mean,\n",
    "                \"ate_std\": bnn_ate_std,\n",
    "                \"ate_ci_95\": bnn_ate_ci_95,\n",
    "                \"ate_ci_90\": bnn_ate_ci_90,\n",
    "                \"ate_ci_width\": bnn_ate_ci_width,\n",
    "                \"ate_samples\": bnn_ate_samples.tolist(),\n",
    "                \"prob_positive_ate\": bnn_prob_positive_ate,\n",
    "                \n",
    "                # Enhanced BNN prediction uncertainty metrics\n",
    "                \"pred_std\": bnn_pred_std,\n",
    "                \"pred_median\": bnn_pred_median,\n",
    "                \"pred_ci_95\": bnn_pred_ci_95,\n",
    "                \"pred_ci_90\": bnn_pred_ci_90,\n",
    "                \"pred_ci_80\": bnn_pred_ci_80,\n",
    "                \"pred_ci_width\": bnn_pred_ci_width,\n",
    "                \"pred_iqr\": bnn_pred_iqr,\n",
    "                \"pred_samples\": bnn_pred_samples.tolist(),\n",
    "                \n",
    "                # Empirical-based BNN ATE metrics (more accurate)\n",
    "                \"ate_ci_95_empirical\": bnn_ate_ci_95_empirical,\n",
    "                \"ate_ci_90_empirical\": bnn_ate_ci_90_empirical,\n",
    "                \"prob_positive_ate_empirical\": bnn_prob_positive_ate_empirical,\n",
    "                \n",
    "                # BNN Training accuracy metrics\n",
    "                \"training_rmse\": bnn_training_rmse,\n",
    "                \"training_r2\": bnn_training_r2,\n",
    "                \"training_mae\": bnn_training_mae,\n",
    "                \n",
    "                # Additional training set statistics\n",
    "                \"training_set_std\": round(np.std(training_true), 3),\n",
    "                \"training_pred_std\": round(np.std(bnn_training_preds), 3),\n",
    "                \n",
    "                # NEW: Data for IoU calculation\n",
    "                \"actual_outcome_ci\": actual_outcome_ci,  # Actual CI from RCT data\n",
    "                \"original_row_index\": index,  # Store original index for potential merging\n",
    "                \n",
    "                # Additional BNN uncertainty measures for comprehensive evaluation\n",
    "                \"pred_range_90\": round(bnn_pred_ci_90[1] - bnn_pred_ci_90[0], 3),\n",
    "                \"pred_range_80\": round(bnn_pred_ci_80[1] - bnn_pred_ci_80[0], 3),\n",
    "            })\n",
    "            \n",
    "            print(f\"{rct_name} - arm: {row['Arm']} - intervention: {trt_outcome}\")\n",
    "            print(f\"  BNN Real outcome: {outcome_control}, Pred: {bnn_predicted_outcome} ± {bnn_pred_std}\")\n",
    "            print(f\"  BNN Pred CI 95%: {bnn_pred_ci_95}, width: {bnn_pred_ci_width}\")\n",
    "            if actual_outcome_ci:\n",
    "                print(f\"  Actual CI: {actual_outcome_ci}\")\n",
    "            print(f\"  BNN Real ATE: {real_ate}, Pred ATE: {bnn_pred_ate} (95% CI: {bnn_ate_ci_95})\")\n",
    "            print(f\"  BNN P(ATE > 0): {bnn_prob_positive_ate} (empirical: {bnn_prob_positive_ate_empirical})\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"BNN Error with {rct_name}: {str(e)}\")\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33dac4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame\n",
    "results_bayesian_bnn_df = pd.DataFrame(results_bayesian_bnn)\n",
    "print(f\"BNN Results DataFrame shape: {results_bayesian_bnn_df.shape}\")\n",
    "\n",
    "# Display the results\n",
    "results_bayesian_bnn_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b50c64c",
   "metadata": {},
   "source": [
    "# Gaussian Process\n",
    "- Custom mean function = multiplier * intervention_outcome. \n",
    "- Multiplier is estimated from Dynamic Ratio Estimation method(from prev. notebook)\n",
    "- Matern32 Kernel\n",
    "\n",
    "Choice of kernel depends on data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6382c174",
   "metadata": {},
   "source": [
    "### More Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9bad920",
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef84524",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities.gp_utils import build_gp_with_informative_prior, predict_gp_with_uncertainty, compute_matern32_kernel\n",
    "from utilities.utils import select_features_with_correlation_analysis, calculate_contextual_prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1241b773",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df['age_risk'] = (training_df['age_median'] > 65).astype(float)\n",
    "training_df['egfr_treatment_match'] = training_df['EGFR_positive_mutation'] * training_df['EGFR_TKI']\n",
    "training_df['immunotherapy_eligible'] = (1 - training_df['brain_metastase_yes']) * training_df['PD1_PDL1_Inhibitor']\n",
    "training_df['vegf_chemo_combo'] = training_df['Anti_VEGF'] * training_df['Chemotherapy']\n",
    "training_df['immuno_targeted_combo'] = training_df['Immunotherapy'] * training_df['Targeted_Therapy']\n",
    "training_df['platinum_taxane_combo'] = training_df['Platinum_Chemotherapy'] * training_df['Taxane']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a6b1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all features\n",
    "list_ftr_selected = ['age_median',\n",
    " 'gender_male_percent',\n",
    " 'ecog_1',\n",
    " 'EGFR_positive_mutation',\n",
    " 'disease_stage_IV',\n",
    " 'disease_stage_recurrent',\n",
    " 'PD1_PDL1_Inhibitor',\n",
    " 'EGFR_TKI',\n",
    " 'Platinum_Chemotherapy',\n",
    " 'Anti_VEGF',\n",
    " 'EGFR_wild',\n",
    " 'disease_stage_III',\n",
    " 'egfr_targeted',\n",
    " 'egfr_tki_use',\n",
    " 'Chemotherapy',\n",
    " 'Immunotherapy',\n",
    " 'vegf_chemo_combo',\n",
    " 'platinum_taxane_combo'\n",
    " ] \\\n",
    "+ [\"age_risk\", \"egfr_treatment_match\", \"immunotherapy_eligible\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a97e368",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pymc as pm\n",
    "import pytensor.tensor as pt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e23bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# MAIN TRAINING LOOP WITH UNCERTAINTY\n",
    "# ========================================\n",
    "\n",
    "print(\"Starting GP with Informative Priors and Full Uncertainty...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "results_gp_uncertainty = []\n",
    "\n",
    "for index, row in training_df.iterrows():\n",
    "    rct_name = row[trial_id_col]\n",
    "    is_arm_control = row[control_arm_col]\n",
    "    \n",
    "    if is_arm_control == 1:\n",
    "        print(f\"\\nProcessing {rct_name}...\")\n",
    "        \n",
    "        # Ground truth\n",
    "        outcome_control = round(row[target_col], 2)\n",
    "\n",
    "        # Get actual CI if available\n",
    "        actual_outcome_ci = None\n",
    "        if \"PFS_median_CI\" in training_df.columns:\n",
    "            actual_ci_value = row[\"PFS_median_CI\"]\n",
    "            if pd.notna(actual_ci_value) and actual_ci_value is not None:\n",
    "                if isinstance(actual_ci_value, (list, tuple, np.ndarray)):\n",
    "                    actual_outcome_ci = actual_ci_value\n",
    "                elif isinstance(actual_ci_value, str):\n",
    "                    try:\n",
    "                        actual_outcome_ci = eval(actual_ci_value)\n",
    "                    except:\n",
    "                        actual_outcome_ci = None\n",
    "        \n",
    "        # Get treatment outcome\n",
    "        trt_arm = training_df.loc[training_df[trial_id_col] == rct_name, :]\n",
    "        trt_outcome = trt_arm.loc[trt_arm[control_arm_col] != 1, target_col]\n",
    "        if trt_outcome.empty or pd.isna(trt_outcome.mean()):\n",
    "            continue\n",
    "        trt_outcome = round(trt_outcome.mean(), 2)\n",
    "        \n",
    "        # Prepare training data (same as before)\n",
    "        training_j = training_df.copy()\n",
    "        training_j[\"is_to_predict\"] = np.where(training_j.index == index, 1, 0)\n",
    "        training_j[\"is_targeted_rct\"] = np.where(training_j[trial_id_col] == rct_name, 1, 0)\n",
    "        \n",
    "        # Add intervention outcomes\n",
    "        for idx, control_row in training_j[training_j[control_arm_col] == 1].iterrows():\n",
    "            ctrl_rct = control_row[trial_id_col]\n",
    "            ctrl_trt_outcome = training_j.loc[(training_j[trial_id_col] == ctrl_rct) & \n",
    "                                             (training_j[control_arm_col] != 1), target_col]\n",
    "            \n",
    "            if not ctrl_trt_outcome.empty and not pd.isna(ctrl_trt_outcome.mean()):\n",
    "                training_j.loc[idx, intervention_outcome_col] = round(ctrl_trt_outcome.mean(), 2)\n",
    "            else:\n",
    "                training_j.loc[idx, intervention_outcome_col] = np.nan\n",
    "        \n",
    "        training_j = training_j.dropna(subset=[intervention_outcome_col])\n",
    "        training_j = training_j.loc[training_j[control_arm_col] == 1, :]\n",
    "        \n",
    "        # Prepare features\n",
    "        training_j = prepare_data(training_j, list_ftr_selected + [\"is_to_predict\", \"is_targeted_rct\", intervention_outcome_col, target_col])\n",
    "        training_j = run_fillna(training_j)\n",
    "        \n",
    "        # Set target intervention outcome\n",
    "        training_j.loc[training_j[\"is_to_predict\"] == 1, intervention_outcome_col] = trt_outcome\n",
    "        \n",
    "        # Extract inference and training data\n",
    "        inference_df = training_j.loc[training_j[\"is_to_predict\"] == 1, :]\n",
    "        if inference_df.empty:\n",
    "            continue\n",
    "            \n",
    "        training_subset = training_j.loc[training_j[\"is_to_predict\"] != 1, :]\n",
    "        training_subset = training_subset.loc[training_subset[\"is_targeted_rct\"] != 1, :]\n",
    "        \n",
    "        if training_subset.shape[0] < 12:\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            # ==========================================\n",
    "            # FEATURE SELECTION\n",
    "            # ==========================================\n",
    "            max_features = min(5, max(3, len(training_subset) // 5))\n",
    "            selected_features = select_features_with_correlation_analysis(\n",
    "                training_subset, list_ftr_selected, target_col, max_features\n",
    "            )\n",
    "            print(f\"  Selected {len(selected_features)} features for {len(training_subset)} samples\")\n",
    "            \n",
    "            # ==========================================\n",
    "            # BUILD GP WITH INFORMATIVE PRIOR\n",
    "            # ==========================================\n",
    "            trace, gp_params = build_gp_with_informative_prior(\n",
    "                training_subset, selected_features, target_col, \n",
    "                intervention_outcome_col, trt_outcome,\n",
    "                draws=400, tune=400\n",
    "            )\n",
    "            \n",
    "            # Check convergence\n",
    "            divergences = trace.sample_stats.diverging.sum().values\n",
    "            if divergences > 20:\n",
    "                print(f\"  Warning: {divergences} divergences, using fallback\")\n",
    "                # Use simple baseline with uncertainty\n",
    "                baseline_pred = 0.712 * trt_outcome\n",
    "                baseline_std = training_subset[target_col].std()\n",
    "                pred_samples = np.random.normal(baseline_pred, baseline_std, size=1000)\n",
    "                pred_samples = np.clip(pred_samples, 0, None)\n",
    "            else:\n",
    "                print(f\"  GP converged with {divergences} divergences\")\n",
    "                \n",
    "                # ==========================================\n",
    "                # POSTERIOR PREDICTIVE SAMPLING\n",
    "                # ==========================================\n",
    "                inference_data = inference_df.iloc[0].to_dict()\n",
    "                pred_samples = predict_gp_with_uncertainty(\n",
    "                    trace, gp_params, inference_data, n_samples=1500\n",
    "                )\n",
    "            \n",
    "            # ==========================================\n",
    "            # UNCERTAINTY QUANTIFICATION\n",
    "            # ==========================================\n",
    "            if len(pred_samples) > 10:\n",
    "                pred_mean = np.mean(pred_samples)\n",
    "                pred_median = np.median(pred_samples)\n",
    "                pred_std = np.std(pred_samples)\n",
    "                \n",
    "                # Confidence intervals\n",
    "                ci_95 = [np.percentile(pred_samples, 2.5), np.percentile(pred_samples, 97.5)]\n",
    "                ci_90 = [np.percentile(pred_samples, 5.0), np.percentile(pred_samples, 95.0)]\n",
    "                ci_68 = [np.percentile(pred_samples, 16.0), np.percentile(pred_samples, 84.0)]\n",
    "                \n",
    "                # Prediction intervals\n",
    "                pred_mean = round(pred_mean, 2)\n",
    "                pred_std = round(pred_std, 2)\n",
    "                \n",
    "                # ATE posterior\n",
    "                ate_samples = trt_outcome - pred_samples\n",
    "                ate_mean = np.mean(ate_samples)\n",
    "                ate_std = np.std(ate_samples)\n",
    "                ate_ci_95 = [np.percentile(ate_samples, 2.5), np.percentile(ate_samples, 97.5)]\n",
    "                \n",
    "                # Probability of positive ATE\n",
    "                prob_positive_ate = np.mean(ate_samples > 0)\n",
    "                \n",
    "            else:\n",
    "                # Fallback\n",
    "                pred_mean = 0.712 * trt_outcome # 0.712 came from dynamic ratio estimate\n",
    "                pred_std = training_subset[target_col].std()\n",
    "                ci_95 = [pred_mean - 1.96*pred_std, pred_mean + 1.96*pred_std]\n",
    "                ci_90 = [pred_mean - 1.64*pred_std, pred_mean + 1.64*pred_std]\n",
    "                ci_68 = [pred_mean - pred_std, pred_mean + pred_std]\n",
    "                ate_mean = trt_outcome - pred_mean\n",
    "                ate_std = pred_std\n",
    "                ate_ci_95 = [ate_mean - 1.96*ate_std, ate_mean + 1.96*ate_std]\n",
    "                prob_positive_ate = 0.5\n",
    "            \n",
    "            # ==========================================\n",
    "            # RESULTS WITH FULL UNCERTAINTY\n",
    "            # ==========================================\n",
    "            real_ate = round(trt_outcome - outcome_control, 2)\n",
    "            \n",
    "            results_gp_uncertainty.append({\n",
    "                \"rct_name\": rct_name,\n",
    "                \"real_ate\": real_ate,\n",
    "                \"pred_ate\": round(trt_outcome - pred_mean, 2),\n",
    "                \"ate_mean\": round(ate_mean, 2),\n",
    "                \"outcome_control\": outcome_control,\n",
    "                \"predicted_mean\": pred_mean,\n",
    "                \"predicted_outcome\": pred_mean,\n",
    "                \"predicted_std\": pred_std,\n",
    "                \"predicted_median\": round(pred_median, 2),\n",
    "                \"ci_95\": [round(ci_95[0], 2), round(ci_95[1], 2)],\n",
    "                \"pred_ci_95\": [round(ci_95[0], 2), round(ci_95[1], 2)],\n",
    "                \"ci_90\": [round(ci_90[0], 2), round(ci_90[1], 2)],\n",
    "                \"ci_68\": [round(ci_68[0], 2), round(ci_68[1], 2)],\n",
    "                \"ate_std\": round(ate_std, 2),\n",
    "                \"ate_ci_95\": [round(ate_ci_95[0], 2), round(ate_ci_95[1], 2)],\n",
    "                \"prob_positive_ate\": round(prob_positive_ate, 3),\n",
    "                \"intervention\": row.get(\"intervention\", \"\"),\n",
    "                \"n_training_samples\": len(training_subset),\n",
    "                \"n_features\": len(selected_features),\n",
    "                \"divergences\": int(divergences),\n",
    "                \"actual_outcome_ci\": actual_outcome_ci,\n",
    "                \"prior_mean\": round(gp_params.get('prior_mean', 0.712 * trt_outcome), 2)\n",
    "            })\n",
    "            \n",
    "            print(f\"  Prediction: {pred_mean:.2f} ± {pred_std:.2f}\")\n",
    "            print(f\"  95% CI: [{ci_95[0]:.2f}, {ci_95[1]:.2f}]\")\n",
    "            print(f\"  ATE: {ate_mean:.2f} ± {ate_std:.2f}\")\n",
    "            print(f\"  P(ATE > 0): {prob_positive_ate:.3f}\")\n",
    "            print(f\"  Actual: {outcome_control}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  Error: {str(e)}\")\n",
    "            continue\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a66954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# ANALYSIS WITH UNCERTAINTY METRICS\n",
    "# ==========================================\n",
    "\n",
    "if results_gp_uncertainty:\n",
    "    results_df = pd.DataFrame(results_gp_uncertainty)\n",
    "    \n",
    "    # Point prediction metrics\n",
    "    rmse = np.sqrt(mean_squared_error(results_df['outcome_control'], results_df['predicted_mean']))\n",
    "    mae = mean_absolute_error(results_df['outcome_control'], results_df['predicted_mean'])\n",
    "    r2 = r2_score(results_df['outcome_control'], results_df['predicted_mean'])\n",
    "    \n",
    "    # Uncertainty calibration\n",
    "    # Check if true values fall within confidence intervals\n",
    "    ci_95_coverage = 0\n",
    "    ci_90_coverage = 0\n",
    "    ci_68_coverage = 0\n",
    "    \n",
    "    for _, row in results_df.iterrows():\n",
    "        true_val = row['outcome_control']\n",
    "        if row['ci_95'][0] <= true_val <= row['ci_95'][1]:\n",
    "            ci_95_coverage += 1\n",
    "        if row['ci_90'][0] <= true_val <= row['ci_90'][1]:\n",
    "            ci_90_coverage += 1\n",
    "        if row['ci_68'][0] <= true_val <= row['ci_68'][1]:\n",
    "            ci_68_coverage += 1\n",
    "    \n",
    "    ci_95_coverage_pct = ci_95_coverage / len(results_df) * 100\n",
    "    ci_90_coverage_pct = ci_90_coverage / len(results_df) * 100\n",
    "    ci_68_coverage_pct = ci_68_coverage / len(results_df) * 100\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"GP WITH UNCERTAINTY QUANTIFICATION RESULTS\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Number of predictions: {len(results_df)}\")\n",
    "    \n",
    "    print(f\"\\nPoint Prediction Performance:\")\n",
    "    print(f\"  RMSE: {rmse:.3f}\")\n",
    "    print(f\"  MAE: {mae:.3f}\")\n",
    "    print(f\"  R²: {r2:.3f}\")\n",
    "    \n",
    "    print(f\"\\nUncertainty Calibration:\")\n",
    "    print(f\"  95% CI coverage: {ci_95_coverage_pct:.1f}% (target: 95%)\")\n",
    "    print(f\"  90% CI coverage: {ci_90_coverage_pct:.1f}% (target: 90%)\")\n",
    "    print(f\"  68% CI coverage: {ci_68_coverage_pct:.1f}% (target: 68%)\")\n",
    "    \n",
    "    print(f\"\\nUncertainty Statistics:\")\n",
    "    print(f\"  Average prediction std: {results_df['predicted_std'].mean():.2f}\")\n",
    "    print(f\"  Average 95% CI width: {(results_df['ci_95'].apply(lambda x: x[1] - x[0])).mean():.2f}\")\n",
    "    \n",
    "\n",
    "else:\n",
    "    print(\"\\nNo results generated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc107ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_bayesian_gp_df = pd.DataFrame.from_dict(results_gp_uncertainty)\n",
    "results_bayesian_gp_df['predicted_outcome'] = results_bayesian_gp_df['predicted_mean']\n",
    "results_bayesian_gp_df['pred_ci_95'] = results_bayesian_gp_df['ci_95']\n",
    "results_bayesian_gp_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee4abf6",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "![Results](../images/rct_lean_results_demo.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7cc6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities.utils import calculate_perf_enhanced_with_iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a5fca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([\n",
    "calculate_perf_enhanced_with_iou(results_bayesian_df.dropna(subset=['real_ate', 'pred_ate']), \"Bayesian LR Approach\",\n",
    "                                  actual_ci_col='actual_outcome_ci',\n",
    "                                  pred_ci_col='pred_ci_95'),\n",
    "calculate_perf_enhanced_with_iou(results_bayesian_bnn_df.dropna(subset=['real_ate', 'pred_ate']), \"Bayesian BNN Approach\",\n",
    "                                  actual_ci_col='actual_outcome_ci',\n",
    "                                  pred_ci_col='pred_ci_95'),\n",
    "calculate_perf_enhanced_with_iou(results_bayesian_gp_df.dropna(subset=['real_ate', 'pred_ate']), \"Bayesian GP Approach\",\n",
    "                                  actual_ci_col='actual_outcome_ci',\n",
    "                                  pred_ci_col='pred_ci_95'),\n",
    "])[['Approach',\n",
    " 'ATE direction true',\n",
    " 'r2_ate',\n",
    " 'spearman_ate',\n",
    " 'rmse_ate',\n",
    " 'r2_outcome',\n",
    " 'rmse_outcome',\n",
    " 'abs_bias_ate',\n",
    " 'avg_ci_width_ate',\n",
    " 'avg_iou_outcome',\n",
    " 'median_iou_outcome'\n",
    "]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e469a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "iou_outcomes_scores = pd.DataFrame([calculate_perf_enhanced_with_iou(results_bayesian_df.dropna(), \"Bayesian LR Approach\",\n",
    "                                  actual_ci_col='actual_outcome_ci',\n",
    "                                  pred_ci_col='pred_ci_95')])\n",
    "iou_outcomes_scores['iou_outcome_scores'].tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10d2f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "iou_results_bayesian_df = results_bayesian_df.dropna().dropna().copy()\n",
    "iou_results_bayesian_df['iou_outcome_scores'] = iou_outcomes_scores['iou_outcome_scores'].tolist()[0]\n",
    "iou_results_bayesian_df[['real_ate', 'pred_ate', 'outcome_control', 'predicted_outcome',\n",
    "                         'rct_name', 'intervention', 'Arm', 'actual_outcome_ci', 'pred_ci_95', 'iou_outcome_scores']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69bcfcd",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "Tree based models still outperforms Bayesian linear model approach with informed prior (**1.08** vs **1.31**). It performs slightly worse than 70% ratio method (**1.20**) (one of our baseline approaches).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd5cefc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
